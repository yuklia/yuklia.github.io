[{"content":" UniFi Dream Machine In this blog post, I\u0026rsquo;ll guide you through the process of setting up a DHCP server on your Synology NAS within a subnet created on a UniFi Dream Machine Special Edition (UDM SE).\nSynology NAS Prerequisites Before we start, ensure you have the following:\nUDM SE: UniFi Dream Machine Special Edition. Synology NAS: Network Attached Storage device from Synology. Raspberry Pi: Any model will suffice. Ethernet cables: To connect your devices. Step-by-Step Guide 1. Create a Subnet on UDM SE Log in to your UniFi Router: Access your UDM SE via the UniFi Network application.\nCreate a New VLAN:\nGo to Settings \u0026gt; Networks \u0026gt; Create New Network. Name your network (e.g., VLAN101). Set the VLAN ID (e.g., 101). Set the Gateway/Subnet to 192.168.101.1/24. Set DHCP Mode to None. Assign Ports to VLAN:\nGo to Settings \u0026gt; Ports. Assign Port 2 and Port 7 to the newly created VLAN 101. (example) 2. Configure Static IP on Synology NAS Connect Your NAS:\nPlug your Synology NAS into Port 7 of your UDM SE. Log in to Your Synology NAS:\nOpen a web browser and log in to the Synology DSM interface. Install DHCP Server Application:\nGo to Package Center. Search for and install the DHCP Server application. Set Static IP Address:\nGo to Control Panel \u0026gt; Network \u0026gt; Network Interface. Select your LAN connection or both if you have connected both Ethernet ports for redundancy. Click on Edit and set a static IP address within your subnet (e.g., 192.168.101.252). 3. Configure DHCP Server on Synology NAS Open DHCP Server Application:\nAfter installation, open the DHCP Server application from the main menu. Enable DHCP Server:\nGo to the DHCP Server application and enable the server. Set the Primary DNS and Secondary DNS (e.g., 8.8.8.8). Create Subnet List: Add a new subnet with the following details: Network Interface: Select the interface connected to VLAN101. Subnet: 192.168.101.0/24. Start IP: 192.168.101.100. End IP: 192.168.101.200. Subnet Mask: 255.255.255.0. Gateway: 192.168.101.1. 4. Connect and Verify Raspberry Pi Connect Raspberry Pi:\nPlug your Raspberry Pi into Port 2 of your UDM SE. Check DHCP Client Logs:\nOn your Synology NAS, go to the DHCP Server application. Check the Logs table. You should see the Raspberry Pi listed as a new DHCP client with an IP address assigned from the 192.168.101.x range. Conclusion Setting up a DHCP server on your Synology NAS within a UDM SE subnet is a straightforward process that offers robust network management capabilities. By following these steps, you can ensure your devices get IP addresses dynamically while maintaining control over your network configuration. If you encounter any issues, ensure all settings are correctly applied and consult the Synology and UniFi documentation for further troubleshooting.\nHappy networking!\n","permalink":"https://juliakostrikova.com/posts/homelab/dhcp-nas/","summary":"UniFi Dream Machine In this blog post, I\u0026rsquo;ll guide you through the process of setting up a DHCP server on your Synology NAS within a subnet created on a UniFi Dream Machine Special Edition (UDM SE).\nSynology NAS Prerequisites Before we start, ensure you have the following:\nUDM SE: UniFi Dream Machine Special Edition. Synology NAS: Network Attached Storage device from Synology. Raspberry Pi: Any model will suffice. Ethernet cables: To connect your devices.","title":"Setting Up a DHCP Server on Synology NAS in a UniFi Dream Machine SE Subnet"},{"content":"RPI 5 on Ubuntu 24.04 (headless): Connect to WiFi / Mobile Hotspot Introduction In my free time, I enjoy tinkering with my home lab, exploring new projects and challenges. Recently, I decided to connect my Raspberry Pi 5 to a mobile hotspot‚Äîa seemingly straightforward task. However, it turned out not so smooth. In this post, I\u0026rsquo;ll walk you through how I resolved the error brcmfmac: brcmf_set_channel: set chanspec fail, reason -52 and successfully connected Raspberry Pi to a mobile hotspot.\nPrerequisites Raspberry Pi 5 Ubuntu 24.04 (headless) Mobile phone with hotspot enabled Access to a terminal or SSH connection Mobile hotspot credentials (SSID and password) Step 1: Create wpa_supplicant.conf Create and configure the wpa_supplicant configuration file to connect to your mobile hotspot.\nvi /etc/wpa_supplicant/wpa_supplicant.conf Add the following content, replacing YourHotspotSSID, YourHotspotPassword, YourContryCode with your actual hotspot credentials.\ncountry=YourContryCode ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\u0026#34;YourHotspotSSID\u0026#34; psk=\u0026#34;YourHotspotPassword\u0026#34; key_mgmt=WPA-PSK } Step 2: Configure Netplan Netplan is used to configure networking on Linux. Create a netplan configuration file.\n‚ÑπÔ∏è netplan Netplan is a utility for easily configuring networking on a linux system. You simply create a YAML description of the required network interfaces and what each should be configured to do. From this description Netplan will generate all the necessary configuration for your chosen renderer tool.\nNetplan reads network configuration from /etc/netplan/*.yaml which are written by administrators, installers, cloud image instantiations, or other OS deployments. During early boot, Netplan generates backend specific configuration files in /run to hand off control of devices to a particular networking daemon.\nNetplan currently works with these supported renderers: NetworkManager Systemd-networkd\nvi /etc/netplan/01-iphone-hotspot-ap.yaml Add the following configuration, substituting wlan0 with your wireless interface name.\nnetwork: version: 2 renderer: networkd wifis: wlan0: dhcp4: yes dhcp6: yes access-points: \u0026#34;YourHotspotSSID\u0026#34;: password: \u0026#34;YourHotspotPassword\u0026#34; Apply the netplan configuration.\nnetplan generate netplan apply --debug Step 3: Restart wpa_supplicant Restart the wpa_supplicant service to apply the new configuration.\nsystemctl restart wpa_supplicant Step 4: Check Connection Status Verify that your device is connected to the mobile hotspot using netplan status.\nnetplan status or ip link show | grep wlan0 üßØ Troubleshooting: Resolving brcmfmac: brcmf_set_channel: set chanspec fail, reason -52 If you encounter the error brcmfmac: brcmf_set_channel: set chanspec fail, reason -52, it may indicate issues with the wireless regulatory domain settings or interference from USB 3.0 devices.\nCheck Regulatory Domain:\nEnsure the regulatory domain is set correctly.\niw reg get iw reg set US # Replace US with your country code To make this persistent, edit the CRDA configuration file.\nvi /etc/default/crda Add or modify the following line:\nREGDOMAIN=US Update Firmware and Drivers:\nEnsure you have the latest firmware and drivers.\napt update apt install --reinstall linux-firmware reboot Conclusion Connecting a headless Ubuntu 24.04 system to a mobile hotspot involves configuring wpa_supplicant and netplan correctly. By following the steps outlined above, you can establish a stable Wi-Fi connection, even in the face of potential issues like the brcmfmac: brcmf_set_channel: set chanspec fail, reason -52 error. Properly setting the regulatory domain and ensuring up-to-date firmware are key to resolving such problems.\nissues to follow:\nhttps://github.com/raspberrypi/linux/issues/6049 https://lore.kernel.org/all/99977c876429f33d8dbab18d7c3e71590585263b.camel@sipsolutions.net/T/ https://forums.raspberrypi.com/viewtopic.php?t=367466 https://github.com/home-assistant/operating-system/issues/3367\nnetplan code spippets:\nhttps://github.com/canonical/netplan/tree/main/examples\nHappy serfing üèÑ :)\nnetplan_status ","permalink":"https://juliakostrikova.com/posts/homelab/mobile-hotspot-on-rpi5/","summary":"RPI 5 on Ubuntu 24.04 (headless): Connect to WiFi / Mobile Hotspot Introduction In my free time, I enjoy tinkering with my home lab, exploring new projects and challenges. Recently, I decided to connect my Raspberry Pi 5 to a mobile hotspot‚Äîa seemingly straightforward task. However, it turned out not so smooth. In this post, I\u0026rsquo;ll walk you through how I resolved the error brcmfmac: brcmf_set_channel: set chanspec fail, reason -52 and successfully connected Raspberry Pi to a mobile hotspot.","title":"RPI 5 on Ubuntu 24.04 (Headless): Connect to WiFi / Mobile Hotspot"},{"content":"Introduction UML diagrams play a crucial role in software engineering for visualizing and structuring information. This article explores the seamless automation of UML diagram creation using ChatGPT (LLM) and the PlantUML tool. The aim is to provide a valuable resource for Solutions Architects and engineers looking to efficiently communicate and pitch ideas through visual representations.\nIn this tutorial, we\u0026rsquo;ll focus on visualization of the high-level system design of Netflix. By the end of this article, you\u0026rsquo;ll have a clear understanding of how ChatGPT and PlantUML can be harnessed to expedite the visualization of systems design while saving time on learning PlatUML syntax.\nStep 1: Gather Requirements In Scope Identification and representation of key Netflix components, such as:\nContent Ingestion Workflow Content Replication Mechanism Content Delivery Networks (CDNs) Data Management Layer Playback Service Steering Service Client Devices Step 2: Compose a Prompt for ChatGPT Prompt Engineer: Create a UML sequence diagram using PlantUML to represent a given algorithm. The output should be generated using the PlantUML language with the *wsd syntax. Here is an algorithm.\n1. Creators kick off the process to upload videos onto the system. 2. The videos get spread out to various networks, aiming to reach users through CDN magic. 3. CDNs shoot back info to the system, covering health checks, file inventory, and the smartest routes for data travel. 4. Video details, along with CDN info, find a cozy spot in the data storage. 5. Users on their devices ask to play stuff, starting a cool workflow for different titles, like movies or shows. 6. The Playback service figures out which bits and pieces are needed for playing the chosen title. 7. The Steering service, using some smart moves, picks the best CDNs for grabbing the necessary files. It then cooks up some fancy URLs and sends them to the user\u0026#39;s device. 8. The user\u0026#39;s device sends out requests to the chosen CDNs, starting the process of grabbing the files. CDNs step up and serve the files requested, making sure everything lands on the user\u0026#39;s device for a good time. Step 3: The *wsd output from ChatGPT @startuml actor Creators actor CDNs actor DataStorage actor Users actor PlaybackService actor SteeringService actor UserDevice note left of Creators Creators kick off the process to upload videos onto the system. end note Creators -\u0026gt; DataStorage: Upload Videos activate DataStorage note left of Creators Videos spread out to various networks, aiming to reach users through CDN magic. end note DataStorage -\u0026gt; CDNs: Distribute Videos activate CDNs CDNs --\u0026gt; DataStorage: Report Info (Health, Inventory, Routes) deactivate CDNs note left of DataStorage Video details and CDN info find a cozy spot in the data storage. end note DataStorage -\u0026gt; CDNs: Persist Video Details and CDN Info note left of Users Users on their devices ask to play stuff, starting a cool workflow for different titles, like movies or shows. end note Users -\u0026gt; PlaybackService: Request to Play activate PlaybackService note left of PlaybackService Playback service figures out which bits and pieces are needed for playing the chosen title. end note PlaybackService -\u0026gt; SteeringService: Determine Components activate SteeringService note left of SteeringService Steering service, using some smart moves, picks the best CDNs for grabbing the necessary files. It then cooks up some fancy URLs and sends them to the user\u0026#39;s device. end note SteeringService -\u0026gt; CDNs: Select Optimal CDNs activate CDNs CDNs --\u0026gt; SteeringService: Optimal CDNs deactivate CDNs SteeringService --\u0026gt; UserDevice: Send Fancy URLs deactivate SteeringService note left of UserDevice User\u0026#39;s device sends out requests to the chosen CDNs, starting the process of grabbing the files. end note UserDevice -\u0026gt; CDNs: Request Files activate CDNs CDNs --\u0026gt; UserDevice: Serve Requested Files deactivate CDNs note left of UserDevice CDNs step up and serve the files requested, making sure everything lands on the user\u0026#39;s device for a good time. end note @enduml Step 4: copy \u0026amp; paste *wsd output to PlantUML renderer I personally use VCCode with PlantUML plugin but you could use any PlantUML renderer you like. VSCode plugin: plantUML Visual result UML sequence Summary In this article I\u0026rsquo;ve outlined a simple and efficient approach to automate UML diagram creation using ChatGPT and PlantUML. The focus is on helping Solutions Architects and engineers quickly visualize some aspects of system designs without getting bogged down by the intricacies of PlantUML syntax\nThe tutorial walks you through a practical example: visualizing the HLD of Netflix. By following 4-steps, you can save time on learning PlantUML syntax and direct your efforts towards creating awesome system designs.\nHappy designing and saving even more time on routine tasks üïê\n","permalink":"https://juliakostrikova.com/posts/chatgpt/plantuml/","summary":"Introduction UML diagrams play a crucial role in software engineering for visualizing and structuring information. This article explores the seamless automation of UML diagram creation using ChatGPT (LLM) and the PlantUML tool. The aim is to provide a valuable resource for Solutions Architects and engineers looking to efficiently communicate and pitch ideas through visual representations.\nIn this tutorial, we\u0026rsquo;ll focus on visualization of the high-level system design of Netflix. By the end of this article, you\u0026rsquo;ll have a clear understanding of how ChatGPT and PlantUML can be harnessed to expedite the visualization of systems design while saving time on learning PlatUML syntax.","title":"Automating UML Diagrams with ChatGPT and PlantUML"},{"content":"My Use case To establish a seamless synchronization of project files between a MacBook (working station) and a Raspberry Pi (hosting server), allowing for automatic updates and accessibility of the project from both devices.\ndevice/tool version DS420+ DSM 7.2-64570 Raspberry PI 400 The user begins working on the project using the IDE on the MacBook, editing and updating project files within the synchronized folder. As changes are made, they are automatically reflected in the synchronized folder and accessible on the Raspberry Pi in read-only mode.\nnasnfs 1. Create folder on NAS 1.1 Set up NFS rules To start with, create a shared folder Control Panel -\u0026gt; Create -\u0026gt; Create Shared Folder Follow all steps though Shared Folder Creation Wizard panel. Once you are done, select your folder (in my case it is yuklia_projects) click on Edit, navigate to NFS permissions tab. Create NFS rule.\nrule value note Hostname or IP raspberrypi_local_ip üîí limit access only to trusted IP to reduce the surface attack. Priviledge Read only read-only since I don‚Äôt need my files to be changed on the raspberryPi side. Squash Map all users to guest Assigns access privileges to all users of NFS client equivalent to the guest access privileges on your system. Security AUTH_SYS Use the NFS client\u0026rsquo;s UID (user identifier) and GID (group identifier) to check access permissions. 2. SSH to raspberryPI 2.1 Connect to your PI ssh username@raspberrypi_local_ip 2.2 Create mount point sudo mkdir /mnt/nas_yuklia_projects 2.3 Mount folder sudo mount -t nfs {SYNOLOGY_IP}:{MOUNT_POINT} /mnt/nas_yuklia_projects MOUNT_POINT - open Control Panel -\u0026gt; Shared Folder -\u0026gt; Edit SharedFolder, navigate to NFS permissions tab, in the left bottom side you\u0026rsquo;ll see Mount path: /volume1/yuklia_projects (in my case)\nSYNOLOGY_IP - local IP address of you Synology device\n3. Validation ‚úÖ Execute df -h to display the list of mounted points; your mounted point should be visible.\n‚úÖ Attempt to modify any files on the Raspberry Pi; you should encounter the warning:\nError writing lock file /mnt/nas_yuklia_projects/.test3.md.swp: Read-only file system 4. Security Measures for NFS üõ°Ô∏è NFS (Network File System) is not inherently secure when used in its basic configuration. It was designed with a focus on ease of use and performance rather than security. However, there are ways to enhance the security of NFS:\n1Ô∏è‚É£ Use NFS Versions with Security Features:\nNFSv4 introduced significant security enhancements compared to older versions (NFSv2 and NFSv3). It includes features like strong authentication and encryption. It\u0026rsquo;s highly recommended to use NFSv4 or later versions for better security.\n2Ô∏è‚É£ Kerberos Authentication:\nImplement Kerberos-based authentication for NFSv4. Kerberos provides secure authentication, preventing unauthorized access to the NFS shares.\n3Ô∏è‚É£ Firewall Configuration:\nUse firewalls to control and restrict access to NFS services. Limit access only to trusted IP addresses or subnets to reduce the attack surface. Network Segmentation:\n4Ô∏è‚É£ Network Segmentation:\nPlace NFS servers on a separate network segment, isolating them from potentially insecure parts of the network. This can help contain security breaches.\n5Ô∏è‚É£ Access Controls:\nUtilize NFS access controls effectively. Set appropriate permissions and access rules on the NFS server to limit access to authorized users and hosts.\n6Ô∏è‚É£ Secure the Underlying Operating System: Ensure that the operating system on both the NFS server and client is securely configured. Regularly apply security patches and updates to address any known vulnerabilities.\n7Ô∏è‚É£ VPN or SSH Tunneling: Use a VPN (Virtual Private Network) or SSH (Secure Shell) tunneling when accessing NFS over untrusted networks. This adds an additional layer of encryption and security.\nSummary In this guide, I show you how to seamlessly sync project files between a MacBook and a Raspberry Pi using Synology NAS and NFS. The step-by-step process ensures updates from the MacBook are automatically reflected on the Raspberry Pi, granting read-only access. Happy syncing!\n","permalink":"https://juliakostrikova.com/posts/homelab/ovderride-provider/","summary":"My Use case To establish a seamless synchronization of project files between a MacBook (working station) and a Raspberry Pi (hosting server), allowing for automatic updates and accessibility of the project from both devices.\ndevice/tool version DS420+ DSM 7.2-64570 Raspberry PI 400 The user begins working on the project using the IDE on the MacBook, editing and updating project files within the synchronized folder. As changes are made, they are automatically reflected in the synchronized folder and accessible on the Raspberry Pi in read-only mode.","title":"How to Mount Synology NAS to RaspberryPi via NFS"},{"content":"S3 is Object-Based Storage Manages data as objects rather then in file systmes or data blocks.\nUpload any file type you can think of to S3. Examples include photos, videos, code, documents, and text files. Cannot be used to run an OS or DB. S3 Basics Unlimited storage. The total amount of data and the number of objects you can store is unlimited. Objects up to 5 TB in Size. S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. S3 Buckets. Store files in buckets (similar to folders). Tired Storage. Offers a range of storage classes designed for different use cases. Lifecycle Management. Defines automatically transition objects to a cheaper tier or delete objects that are no longer required after a set of period of time. Versioning. All versions of an object are stored and can be retrived, including deleted objects. Once enabled, versioning cannot be disabled - only suspended. Supports MFA, so you need to procced with 2-factor in order to delete an object. If you enabled public access to versioned objects, old versions will not be accessible. Working with S3 Buckets Universal Namespace. All AWS accounts share the S3 namespace. Each S3 bucket name is globally unique.\nExample S3 URLs\nhttps://bucket-name.s3.Region.amazonaws.com/key-name Key-Value Store Key the name of object (e.g., Toba.jpg) Value the data itself, which is made up of a sequence of bytes Version ID important for storing multiple versions of the same object Metadata data about the data you are storing (e.g., content-type, last-modified, etc.)\nHight Available and Highly Durable build for 99.95 - 99.99% service availability, depending on the S3 tier. designed for 99.999999999 (9 decimal places) durability for data stored in S3. data stored redundantly across multiple devices in multiple facilities Securing your data üîí Buckets are private by default. You have to apply public access on both the bucket and its objects in order to make the bucket public.\nEncryption üîë 1.1 Encryption in Transit\nSSl/TLS HTTPS 1.2 Encryption at Rest: Server-Side Encryption\nSSE-S3 : S3-managed keys, using AES 256-bit encryption SSE-KMS : AWS Key Management Service-managed keys SSE-C : Customer-provided keys 1.3 Encryption at Rest: Client-Side Encryption\nencrypt you files yourself before uploading to S3 Access Control List (ACLs) Define which AWS accounts or groups are granted access and the type of access. You can attach S3 ACLs to individial objects within a bucket. Bucket Policies. S3 bucket policies specify what actions are allowed or denied (e.g., allow user Alice to PUT but not DELETE objects in the bucket.) Bucket policies work on an entire bucket level. Enforcing Server-Side Encryption 1Ô∏è‚É£ Console. Select encryption settings in S3 Bucket.\n2Ô∏è‚É£ Bucket Policy.\nx-amz-server-side-encryption: AES256 - (SSE-S3 - S3 managed keys)\nx-amz-server-side-encryption: aws:kms - (SSE-KMS - KMS managed keys)\nüí° You can create abucket policy that denies any S3 PUT request that doesn\u0026rsquo;t include the x-amz-server-side-encryption parameter in request header.\nPUT request\nPUT /my-image.jpg HTTP/1.1 Host: myBucket.s3.\u0026lt;Region\u0026gt;.amazonaws.com Date: Wed, 12 Oct 2009 17:50:00 GMT Authorization: authorization string Content-Type: text/plain Content-Length: 11434 x-amz-meta-author: Janet Expect: 100-continue x-amz-server-side-encryption: AES256 [11434 bytes of object data] Static Websites on S3 S3 scales automatically to meet demand Many enterprises will put static websites on S3 when they think there is going to be a large number or requests.\nS3 Storage Classes S3 Standard data is stored redunduntly across multiple devices in multiple facilities (\u0026gt;= 3 AZs) designed for frequent access suitable for most workloads: the default storage class; use cases include websites, content distribution, mobile and gaming applications, and big data analatics. 99.99% availability 99.999999999 (11 9\u0026rsquo;s) durability S3 Standard-Infrequent Access (S3 Standard-IA) Use Case: great for long-term storage, backups and as data store for disaster recovery files\nrapid access used for data that is accessed less frequently but requires rapid access when needed. you pay to access data there is low per-GB storage price and a per-GB retrieved fee. 99.9% availability 99.999999999 (11 9\u0026rsquo;s) durability S3 One Zone Infrequent Access Use Case: good for long-lived, infrequently accessed, non-critical data\nLike S3 Standard-IA, but data is stored redunduntly within single AZ.\ncost 20% less then regular S3 Standard-IA 99.5% availability 99.999999999 (11 9\u0026rsquo;s) durability S3 Intelligent-Tiering Use Case: If you don;t know whether you\u0026rsquo;ll be accessing data frequently or infrequently\nüíµ optimization: montly fee of $0.0025 per 1,000 objects\nautomatically moves your data to the most cost-effective tier based on how frequently you access each object. 99.99% availability 99.999999999 (11 9\u0026rsquo;s) durability 3 Glacier Options you pay each time you access data use only for archieving data glacier is cheap storage optimized for data that is very infrequently accessed 99.99% availability 99.999999999 (11 9\u0026rsquo;s) durability Option 1: Glacier Instant Retrieval provides long-term data archiving with instant retrieval time for your data.\nOption 2: Glacier Flexible Retrieval ideal storage class for archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases. can be minites or up to 12 hours.\nOption 3: Glasier Deep Archive cheapest storage class and designed for customers that retain data sests for 7-10 years or longer to meet customer needs and regulatory compliance requirements. the standard retrieval time is 12 hours and the bulk retrieval time is 48 hours.\nPerformance across the S3 Storage Classes S3 Standard S3 Intelligent-Tiering S3 Standard-IA S3 One Zone-IA+ S3 Glacier Instant Retrieval S3 Glacier Flexible Retrieval S3 Glacier Deep Archive Designed for Durability 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) Designed for Availability 99.99% 99.90% 99.90% 99.50% 99.99% 99.99% 99.99% Availability SLA 99.90% 99% 99% 99% 99.90% 99.90% 99.90% AZs \u0026gt;=3 \u0026gt;=3 \u0026gt;=3 1 \u0026gt;=3 \u0026gt;=3 \u0026gt;=3 Min Capacity charge per Object n/a n/a 128KB 128KB 128KB 40KB 40KB Min Storage duration charge n/a 30 days 30 days 30 days 90 days 90 days 180 days Retrieval Fee n/a n/a Per GB retrieved Per GB retrieved Per GB retrieved Per GB retrieved Per GB retrieved Storage Type Object Object Object Object Object Object Object Lifecycle Transitions Yes Yes Yes Yes Yes Yes Yes Storage Classes - Costs üíµ S3 Standard general-purpose for any type of data, typically used for frequent access\nFirst 50 TB / Month 0.023$ per GB Next 450 TB / Month 0.022$ per GB Over 500 TB / Month 0.021$ per GB S3 Intelligent-Tiering cost saving strategies applied for data with unknown or changing access patterns\nno min storage duration\nFirst 50 TB / Month 0.023$ per GB Next 450 TB / Month 0.022$ per GB Over 500 TB / Month 0.021$ per GB All Storage / Month 0.0025$ per GB Monitoring and Automation, All Storage / Month 0.0025$ per 1000 Objects S3 Standard Infrequent Access long-lived but infrequently accessed data (once a month) that needs milliseconds access 30 days min storage duration\nAll Storage / Month 0.0125$ per GB S3 One Zone-Infrequent Access re-creatable infrequently accessed data that needs milliseconds access 30 days min storage duration\nAll Storage / Month 0.01$ per GB S3 Glacier long-lived archived data accssed once a quarter with instant retrieval in milliseconds\nAll Storage / Month 0.004$ per GB S3 Glacier Flexible Retrieval archived data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost. retrieval time: can be minutes or up to 12 hours 90 days min storage duration\nAll Storage / Month 0.0036$ per GB S3 Glacier Deep Archive designed to retain data sets for 7-10 years or longer. retrieval time: 12 hours and the bulk is 48 hours 180 days min storage duration\nAll Storage / Month 0.00099$ per GB Lifecycle Management automate moving object between the different storage tiers, thereby maximazing cost effectiveness. can be used in conjuction with versioning can be applied to current and noncurrent versions Example: S3 Standard: keep for 30 days -\u0026gt; S3 IA: after 30 days -\u0026gt; Glacier: After 90 days\nS3 Object Lock üîí you can use S3 Object Lock to store objects using a write once, read many model. It can help to prevent objects from being delete or modified for a fixed amount of time or indefinitely. object lock can be on individual objects or applied across the bucket as a whole object lock comes in two modes: Governance Mode, Complience Mode S3 Object Lock Modes Governance Mode\nUsers can\u0026rsquo;t overwrite or delete an object version or alter its lock settings unless they have special permissions.\nComplience Mode\nA protected object version can\u0026rsquo;t be overwritten or deleted by any user, including the root in your AWS Account. Retention mode of object can\u0026rsquo;t be changed and its retenton period can\u0026rsquo;t be shortened. Compiance mode ensures an object version can\u0026rsquo;t be overriten or deleted for the duration of the retention period.\nRetention Periods üï• Retention period protects an object version for a fixed amount of time. Ehen ypu place a retention period on an object version, Amazon S3 stores a timestamp in the object version\u0026rsquo;s metadata to indicate when the retention period expires.\nüí° After retention period expires, the object version can be overriten or deleted unless you also placed a legal hold on object version.\nLegal Holds Like retention period, a legal hold prevents an object version from being overriten or deleted.\nHowever, a legal holds doesn\u0026rsquo;t have an assosiated retention period and remains in effect untill removed.\nA Legal Hold can be placed and removed by any user who has s3:PutObjectLegalHold permission.\nGlacier Vault Lock Allows to deploy and enforce compiance controls for individual S3 Glacier vaults with a vault lock policy.\nyou can specify controls such as WORM, in a vault lock policy.\nüî¥ once locked, the policy can\u0026rsquo;t be changed\nOptimizing S3 Performance S3 Prefixes this is folders inside you bucket. it doesn\u0026rsquo;t include object name\nbucketname/folder1/subfolder1/file.md -\u0026gt; prefix: /folder1/subfolder1/ bucketname/folder2/subfolder1/file.md -\u0026gt; prefix: /folder2/subfolder1/ bucketname/folder3/file.md -\u0026gt; prefix: /folder3 S3 Performance S3 has low latency. You can get the first byte out of S3 within 100-200 milliseconds.\nYou can achive a hight number of requests:\n3,500 PUT/COPY/POST/DELETE 5,500 GET/HEAD requests per second, pre prefix.\nüí° the more prefixes you have on the bucket, the higher performance you can get. for example, 2 prefixes gives you 11,000 rps; 4 prefixes = 22,000; etc.\nLimitations with KMS üü† SSE-KMS; with file upload, you will call GenerateDataKey in the KMS API. -\u0026gt; KMS quota (can\u0026rsquo;t be increased)\nüü† SSE-KMS; with file download you will call Decrypt in the KMS API. -\u0026gt; KMS quota (can\u0026rsquo;t be increased)\nbetter to use native S3 encryption (SSE-S3) rather then KMS\nUploads üü¢ Multipart Uploads\nrecommended for files over 100 MB required for files over 5 GB parallelize uploads (increase efficiency) Downloads üü¢ S3 Byte-Range Fetches\nparallelize downloads by specifying byte ranges. if there is a failure in the download, it it\u0026rsquo;s only for a specific byte range. Backup Data with S3 Replication you can replicate objects from one bucket to another.versioning must be enabled for both sides objects in an existing bucket are not replicated automatically delete markers are not replicated by default ","permalink":"https://juliakostrikova.com/posts/saa-c03/s3/","summary":"S3 is Object-Based Storage Manages data as objects rather then in file systmes or data blocks.\nUpload any file type you can think of to S3. Examples include photos, videos, code, documents, and text files. Cannot be used to run an OS or DB. S3 Basics Unlimited storage. The total amount of data and the number of objects you can store is unlimited. Objects up to 5 TB in Size. S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB.","title":"SAA-C03:: Simple Storage Service (S3)"},{"content":"Virtualization The technology that lies at the core of all cloud operations is virtualization. Virtualization lets you divide the hardware resources of a single physical server into smaller units. That physical server could therefore host multiple virtual machines (VMs) running their own complete operating systems, each with its own memory, storage, and network access.\na virtual machine host Cloud Computing Major cloud providers like AWS have enormous server farms where hundreds of thousands of servers and disk drives are maintained along with the network cabling necessary to connect them. A well-built virtualized environment could provide a virtual server using storage, memory, compute cycles, and network bandwidth collected from the most efficient mix of available sources it can find. A cloud computing platform offers on-demand, self-service access to pooled compute resources where your usage is metered and billed according to the volume you consume. Cloud computing systems allow for precise billing models, sometimes involving fractions of a penny for an hour of consumption.\nAWS Global Infrastructure At the time of writing this note, The AWS Cloud spans ‚¨áÔ∏è\n102 Availability Zones within 32 geographic regions around the world 400+ Edge Locations and 13 Regional Edge Caches üí° with announced plans for 15 more Availability Zones and 5 more AWS Regions in Canada, Israel, Malaysia, New Zealand, and Thailand.\nAvailability Zones / Data Centers An Availability Zone (AZ) is a data center. AZ is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region.\nAll AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. All traffic between AZs is encrypted.\nIf an application is partitioned across AZs, companies are better isolated and protected from issues such as\npower outages, lightning strikes, tornadoes, earthquakes, and more. AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles) of each other.\nRegions A region is a geographical area. Each Region consists of 2 (or more) Availability Zones.\nEdge Networking Edge networking in AWS refers to the concept of distributing content, applications, and services closer to end-users or devices. This approach aims to reduce latency and improve performance by minimizing the distance data needs to travel. AWS offers several services and features that support edge networking, such as\nAmazon CloudFront, AWS Global Accelerator, Amazon Route 53. These services reside at AWS‚Äô global edge locations connected by dedicated 100Gbps redundant fiber .\nAmazon CloudFront This is a content delivery network (CDN) service that caches and delivers content, including\nweb pages, images, videos, other assets from multiple edge locations around the world. It helps ensure that users can access content with minimal delay, regardless of their geographical location.\nAWS Global Accelerator This service allows you to direct traffic over the AWS global network infrastructure, dynamically routing requests to the nearest and most optimal AWS endpoint. It improves the availability and performance of applications by leveraging Anycast routing.\nIn Anycast routing, multiple servers or network nodes are configured with the same IP address. When a user sends a request to that IP address, the network routes the request to the nearest available node using routing protocols and metrics such as network topology, latency, and other factors.\nAmazon Route 53 AWS\u0026rsquo;s domain name system (DNS) service, Route 53, can be used to route traffic based on geographic location, latency, health, or other routing policies. This helps optimize the distribution of traffic and ensures efficient access to resources.\nIn fact, it focuses on four distinct areas:\nDomain registration DNS management Availability monitoring (health checks) Traffic management (routing policies) Route 53 now also provides an Application Recovery Controller through which you can configure recovery groups, readiness checks, and routing control.\nSecurity and Identity Identity and Access Management (IAM) You use IAM to administer user and programmatic access and authentication to your AWS account. Through the use of users, groups, roles, and policies, you can control exactly who and what can access and/or work with any of your AWS resources.\nKey Management Service (KMS) KMS is a managed service that allows you to administrate the creation and use of encryption keys to secure data used by and for any of your AWS resources.\nDirectory Service For AWS environments that need to manage identities and relationships, Directory Service can integrate AWS resources with identity providers like Amazon Cognito and Microsoft AD domains.\nCompute EC2 The central focus within a conventional datacenter or server room revolves around its invaluable servers. However, to harness the utility of these servers, one must incorporate elements such as racks, power supplies, cabling, switches, firewalls, and cooling systems. Amazon Web Services\u0026rsquo; (AWS) Elastic Compute Cloud (EC2) is meticulously crafted to faithfully emulate the experience of a datacenter or server room. At its core lies the EC2 virtual server, referred to as an \u0026ldquo;instance.\u0026rdquo; Yet, akin to the scenario of a local server room described earlier, AWS offers an array of services meticulously designed to bolster and refine the operations of your workloads. These encompass tools for overseeing and managing resources, alongside purpose-built platforms catering to the coordination of containers.\nLambda At its core, AWS Lambda allows developers to encapsulate discrete pieces of functionality into self-contained units known as functions. These functions can be triggered by a multitude of events, ranging from HTTP requests and changes to data stored in Amazon S3, to updates in databases and modifications in the AWS Management Console. Each function executes in a completely isolated environment, with its own compute resources and dependencies, ensuring optimal performance and scalability.\nElastic Beanstalk Beanstalk is a managed service that abstracts the provi- sioning of AWS compute and networking infrastructure. You are required to do nothing more than push your application code, and Beanstalk automatically launches and manages all the necessary services in the background.\nAuto Scaling Copies of running EC2 instances can be defined as image templates and automatically launched (or scaled up) when client demand can‚Äôt be met by existing instances. As demand drops, unused instances can be terminated (or scaled down).\nElastic Load Balancing Incoming network traffic can be directed between multiple web servers to ensure that a single web server isn‚Äôt overwhelmed while other servers are underused or that traffic isn‚Äôt directed to failed servers.\nElastic Container Service Compute workloads using container technologies like Docker and Kubernetes can be provisioned, automated, deployed, and administered using full integration with your other AWS account resources. Kubernetes workloads have their own environment: Amazon Elastic Kubernetes Service (EKS).\nStorage S3 S3 offers highly versatile, reliable, and inexpensive object storage that‚Äôs great for data storage and backups. It‚Äôs also commonly used as part of larger AWS production processes, including through the storage of script, template, and log files.\nS3 Glacier A good choice for when you need large data archives stored cheaply over the long term and can live with retrieval delays measuring in the hours. Glacier‚Äôs life cycle management is closely integrated with S3.\nEBS EBS provides the persistent virtual storage drives that host the operating systems and working data of an EC2 instance. They‚Äôre meant to mimic the function of the storage drives and partitions attached to physical servers.\nEBS-Provisioned IOPS SSD If your applications will require intense rates of I/O operations, then you should consider provisioned IOPS. There are currently three flavors of provisioned IOPS volumes: io1, io2, and io2 Block Express. io1 can deliver up to 50 IOPS/GB to a limit of 64,000 IOPS (when attached to an AWS Nitro‚Äìcompliant EC2 instance) with a maximum throughput of 1,000 MB/s per volume. io2 can provide up to 500 IOPS/GB. And io2 Block Express can give you 4,000 MB/s throughput and 256,000 IOPS/volume.\nEBS General-Purpose SSD For most regular server workloads that, ideally, deliver low-latency performance, general- purpose SSDs will work well. You‚Äôll get a maximum of 3,000 IOPS/volume. For refer- ence, assuming 3,000 IOPS per volume and a single daily snapshot, a general-purpose SSD used as a typical 8 GB boot drive for a Linux instance would, at current rates, cost you $3.29/month.\nHDD Volumes For large data stores where quick access isn‚Äôt important, you can find cost savings using older spinning hard drive technologies. The sc1 volume type provides the lowest price of any EBS storage ($0.015/GB-month). Throughput optimized hard drive volumes (st1) are available for larger stores where infrequent bursts of access at a rate of 250 MB/s per TB are sufficient. st1 volumes cost $0.045/GB per month. The EBS Create Volume dialog box cur- rently offers a Magnetic (Standard) volume type.\nEFS The Elastic File System (EFS) provides automatically scalable and shareable file storage to be accessed from Linux instances. EFS-based files are designed to be accessed from within a virtual private cloud (VPC) via Network File System (NFS) mounts on EC2 Linux instances or from your on-premises servers through AWS Direct Connect connections. The goal is to make it easy to enable secure, low-latency, and durable file sharing among multiple instances.\nFSx Amazon FSx comes in four flavors: FSx for Lustre, FSx for Windows File Server, FSx for OpenZFS, and FSx for NetApp ONTAP. Lustre is an open source distributed filesystem built to give Linux clusters access to high-performance filesystems for use in compute-intensive operations. As with Lustre, Amazon‚Äôs FSx service brings OpenZFS and NetApp filesystem capabilities to your AWS infrastructure. FSx for Windows File Server, as you can tell from the name, offers the kind of file-sharing service EFS provides but for Windows servers rather than Linux. FSx for Windows File Server integrates operations with Server Message Block (SMB), NTFS, and Microsoft Active Directory.\nStorage Gateway Storage Gateway is a hybrid storage system that exposes AWS cloud storage as a local, on-premises appliance. Storage Gateway can be a great tool for migration and data backup and as part of disaster recovery operations.\nAWS Snow Family Migrating large data sets to the cloud over a normal Internet connection can sometimes require far too much time and bandwidth to be practical. If you‚Äôre looking to move terabyte- or even petabyte-scaled data for backup or active use within AWS, ordering a Snow device might be the best option. When requested, AWS will ship you a physical, appropriately sized device to which you transfer your data. When you‚Äôre ready, you can then send the device back to Amazon. Amazon will then transfer your data to buckets on S3.\nAWS DataSync DataSync specializes in moving on-premises data stores into your AWS account with a minimum of fuss. It works over your regular Internet connection, so it‚Äôs not as useful as Snowball for really large data sets. But it is much more flexible, since you‚Äôre not limited to S3 (or RDS as you are with AWS Database Migration Service). Using DataSync, you can drop your data into any service within your AWS account. That means you can do the following: Quickly and securely move old data out of your expensive datacenter into cheaper S3 or Glacier storage. Transfer data sets directly into S3, EFS, or FSx, where it can be processed and analyzed by your EC2 instances. Apply the power of any AWS service to any class of data as part of an easy-to-configure automated system. A single DataSync task can handle external transfer rates of up to 10 Gbps (assuming your connection has that capacity) and offers both encryption and data validation.\nDatabases RDS Amazon Relational Database Service (RDS) is a managed database service that lets you run relational database systems in the cloud. RDS takes care of setting up the database system, performing backups, ensuring high availability, and patching the database software and the underlying operating system. RDS also makes it easy to recover from database failures, restore data, and scale your databases to achieve the level of performance and availability that your application requires.\nDynamoDB DynamoDB is a managed nonrelational database service that can handle thousands of reads and writes per second. It achieves this level of performance by spreading your data across multiple partitions. A partition is a 10 GB allocation of storage for a table, and it‚Äôs backed by solid-state drives in multiple availability zones.\nRedshift Redshift is Amazon‚Äôs managed data ware- house service. Although it‚Äôs based on PostgreSQL, it‚Äôs not part of RDS. Redshift uses columnar storage, meaning that it stores the values for a column close together. This improves storage speed and efficiency and makes it faster to query data from individual col- umns.\nNetworking VPCs Like a traditional network, a VPC consists of at least one range of contiguous IP addresses. This address range is represented as a Classless Inter-Domain Routing (CIDR) block. The CIDR block determines which IP addresses may be assigned to instances and other resources within the VPC. You must assign a primary CIDR block when creating a VPC. After creating a VPC, you divide the primary VPC CIDR block into subnets that hold your AWS resources.\nDirect Connect The AWS Direct Connect service uses PrivateLink to offer private, low-latency connec- tivity to your AWS resources. One advantage of Direct Connect is that it lets you bypass the Internet altogether when accessing AWS resources, letting you avoid the unpredictable and high latency of a broadband Internet connection. This approach is useful when you need to transfer large data sets or real-time data, or you need to meet regulatory requirements that preclude transferring data over the Internet.\nApplication Integration API Gateway If you need to build an application that consumes data or other resources existing within the AWS platform, you can make reliable and safe connections using either RESTful or WebSocket APIs. API Gateway lets you publish data coming out of just about any AWS ser- vice for use in your IoT device fleets, web or mobile applications, or monitoring dashboards.\nSimple Notification Service (SNS) SNS is a notification tool that can automate the publishing of alert topics to other services (to an SQS Queue or to trigger a Lambda function, for instance), to mobile devices, or to recipients using email or SMS.\nSimple Workflow (SWF) SWF lets you coordinate a series of tasks that must be per-formed using a range of AWS services or even nondigital (meaning human) events.\nSimple Queue Service (SQS) SQS allows for event-driven messaging within distributed systems that can decouple while coordinating the discrete steps of a larger process.The data contained in your SQS messages will be reliably delivered, adding to the fault- tolerant qualities of an application.\nApplication management CloudWatch CloudWatch can be set to monitor process performance and resource utilization and, when preset thresholds are met, either send you a message or trigger an automated response.\nCloudFormation This service enables you to use template files to define full and complex AWS deployments. The ability to script your use of any AWS resources makes it easier to automate, stan- dardizing and speeding up the application launch process.\nCloudTrail CloudTrail collects records of all your account‚Äôs API events. This history is useful for account auditing and troubleshooting purposes.\nConfig The Config service is designed to help you with change management and compliance for your AWS account. You first define a desired configuration state, and Config evalu- ates any future states against that ideal. When a configura- tion change pushes too far from the ideal baseline, you‚Äôll be notified.\nAWS Well-Architected Framework 1. Operational excellence Perform operations as code Make frequent, small, reversible changes Refine operations procedures frequently Anticipate failure Learn from all operational failures 2. Security Implement a strong identity foundation (least privilege principle) Maintain traceability Apply security at all layers Automate security best practices Implementation of controls that are defined and managed as code in version-controlled templates\nProtect data in transit and at rest Keep people away from data Prepare for security events Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery\n3. Reliability Automatically recover from failure Test recovery procedures Scale horizontally to increase aggregate workload availability Replace one large resource with multiple small resources to reduce the impact of a single failure on the overall workload. Distribute requests across multiple, smaller resources to verify that they don‚Äôt share a common point of failure.\nStop guessing capacity In on-premises scenarios, resource saturation leading to workload failure is often caused by exceeding capacity, while in the cloud, monitoring and automated adjustments of resources enable efficient demand satisfaction without over- or under-provisioning, within certain limits and managed quotas.\nManage change in automation 4. Performance efficiency Democratize advanced technologies Make advanced technology implementation smoother for your team by delegating complex tasks to your cloud vendor\nGo global in minutes Use serverless architectures Experiment more often Consider mechanical sympathy Understand how cloud services are consumed and always use the technology approach that aligns with your workload goals. For example, consider data access patterns when you select database or storage approaches.\n5. Cost optimization Implement Cloud Financial Management Adopt a consumption model Measure overall efficiency Stop spending money on undifferentiated heavy lifting Analyze and attribute expenditure You can use cost allocation tags to categorize and track your AWS usage and costs. When you apply tags to your AWS resources (such as EC2 instances or S3 buckets), AWS generates a cost and usage report with your usage and your tags.\n6. Sustainability Understand your impact Establish sustainability goals Maximize utilization Anticipate and adopt new, more efficient hardware and software offerings Use managed services Reduce the downstream impact of your cloud workload References\nhttps://www.wellarchitectedlabs.com\n","permalink":"https://juliakostrikova.com/posts/saa-c03/aws-fundamentals/","summary":"Virtualization The technology that lies at the core of all cloud operations is virtualization. Virtualization lets you divide the hardware resources of a single physical server into smaller units. That physical server could therefore host multiple virtual machines (VMs) running their own complete operating systems, each with its own memory, storage, and network access.\na virtual machine host Cloud Computing Major cloud providers like AWS have enormous server farms where hundreds of thousands of servers and disk drives are maintained along with the network cabling necessary to connect them.","title":"SAA-C03:: Introduction to Cloud Computing and AWS"},{"content":"Traefik v1.7 is running within a Docker Swarm, functioning as a global service. Consequently, an instance of Traefik is deployed for each host within our Docker Swarm cluster. It is important to note that the Swarm\u0026rsquo;s routing mesh is not employed in this setup. Instead, the port mode is configured as \u0026ldquo;host,\u0026rdquo; allowing the port to be attached to the host\u0026rsquo;s network.\ntraefik: image: ${REGISTRY}/traefik:1.6 build: ./traefik volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - traefik environment: - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} - AWS_REGION=${AWS_REGION} - AWS_HOSTED_ZONE_ID=${AWS_HOSTED_ZONE_ID} ports: - target: 80 published: 80 protocol: tcp mode: host - target: 443 published: 443 mode: host protocol: tcp - target: 8080 published: 8080 mode: host protocol: tcp deploy: mode: global restart_policy: condition: on-failure Docker Swarm deployed to AWS and lives behind internal/external load balancers (Classic type).\n// terraform SG fragment ingress { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;acme chellange\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } ingress { from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;SSL termination\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } Let\u0026rsquo;s assume mydomain.com/mobile-api should be worldwide visible but the rest of app should be available via Whitelist. Hopefully Traefik supports entrypoints by route, so there is nothing easiest then make one more clone of app and set frontend rule targeting to /route.\nmodile-api: image: ${REGISTRY}/app:${VERSION:-latest} networks: - traefik deploy: mode: replicated replicas: 1 restart_policy: condition: on-failure labels: - traefik.backend=modile-api - traefik.port=80 - traefik.frontend.rule=Host:${HOST_API};PathPrefix:/mobile-api - traefik.docker.network=traefik - traefik.enable=true app: image: ${REGISTRY}/app:${VERSION:-latest} networks: - traefik deploy: mode: replicated replicas: 1 restart_policy: condition: on-failure labels: - traefik.backend=app - traefik.port=80 - traefik.frontend.whiteList.sourceRange=${WHITE_LIST} - traefik.frontend.whiteList.useXForwardedFor=true - traefik.frontend.rule=Host:${HOST_API} - traefik.docker.network=traefik - traefik.enable=true Important part: set up ProxyProtocol on both side Traefik (reverse-proxy) and AWS ELB. Proxy protocol Traefik side: traefik.toml\n[entryPoints.https.whiteList] sourceRange = [\u0026#34;1.2.3.4/24\u0026#34;] # you can override this part on service level useXForwardedFor = true [entryPoints.https.proxyProtocol] trustedIPs = [\u0026#34;10.0.100.0/24\u0026#34;] # private subnet CIRD block or IPs [entryPoints.https.forwardedHeaders] trustedIPs = [\u0026#34;10.0.100.0/24\u0026#34;] # private subnet CIRD block or IPs Proxy protocol AWS side: (if your listener settings support Proxy Protocol! )\n1 # make sure proxyPolicy is disabled. =\u0026gt; \u0026#34;BackendServerDescriptions\u0026#34; : [] aws elb describe-load-balancer-policy-types 2 aws elb create-load-balancer-policy --load-balancer-name %lb_name% --policy-name %name% --policy-type-name ProxyProtocolPolicyType --policy-attributes AttributeName=ProxyProtocol,AttributeValue=true 3 aws elb set-load-balancer-policies-for-backend-server --load-balancer-name %lb_name% --instance-port 443 --policy-names %policy_name% 4 check (optional) aws elb describe-load-balancers --load-balancer-name %lb_name% debug: aws elb delete-load-balancer-policy --load-balancer-name %lb_name% --policy-name %policy_name% # get ELB IPs aws ec2 describe-network-interfaces --filters Name=description,Values=\u0026#34;ELB elb_name\u0026#34; --query \u0026#39;NetworkInterfaces[*].PrivateIpAddresses[*]\u0026#39; If everything goes as expected you‚Äôll see your IP in X-Forwarded-For header üéâ\nHostname: 6b27211a260c IP: 127.0.0.1 IP: 10.255.1.59 IP: 172.18.0.13 IP: 10.0.2.65 GET / HTTP/1.1 Host: domain.com User-Agent: redacted Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3 Accept-Encoding: gzip, deflate Accept-Language: en-US,en;q=0.9,ru-RU;q=0.8,ru;q=0.7,uk-UA;q=0.6,uk;q=0.5 Cache-Control: max-age=0 Connection: keep-alive Upgrade-Insecure-Requests: 1 X-Forwarded-For: you_real_ip X-Forwarded-Port: 80 X-Forwarded-Proto: https ","permalink":"https://juliakostrikova.com/posts/cloud/traefik-x-forwarded-for/","summary":"Traefik v1.7 is running within a Docker Swarm, functioning as a global service. Consequently, an instance of Traefik is deployed for each host within our Docker Swarm cluster. It is important to note that the Swarm\u0026rsquo;s routing mesh is not employed in this setup. Instead, the port mode is configured as \u0026ldquo;host,\u0026rdquo; allowing the port to be attached to the host\u0026rsquo;s network.\ntraefik: image: ${REGISTRY}/traefik:1.6 build: ./traefik volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - traefik environment: - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} - AWS_REGION=${AWS_REGION} - AWS_HOSTED_ZONE_ID=${AWS_HOSTED_ZONE_ID} ports: - target: 80 published: 80 protocol: tcp mode: host - target: 443 published: 443 mode: host protocol: tcp - target: 8080 published: 8080 mode: host protocol: tcp deploy: mode: global restart_policy: condition: on-failure Docker Swarm deployed to AWS and lives behind internal/external load balancers (Classic type).","title":"Traefik behind AWS ELB. X-Forwarded-For header"},{"content":"What you will learn today ü¶¶: what is Traefik and why I decisively recommend to use it with Docker Swarm how to show custom error page if Traefik returns 404 (service not found) How I met Traefik üòÅ Traefik is a top modern reverse proxy and load balancer. I started to use it from major version 1. At that moment, I already had Docker Swarm cluster, which I began to prepare for production use, and only one thing in the coherent picture has been left: \u0026ldquo;External Load Balancer\u0026rdquo;. Official docker doc had an example with HaProxy so it was obvious to try it out first. As I remember it took a significant amount of time to get HaProxy works smoothly with Swarm. But I wasn\u0026rsquo;t satisfied with the result:\nstatic config which is needed to re-deploy in case of changes *.cfg not supports variables substitution (out of the box) no native support of Lets\u0026rsquo;s Encrypt The production deadline was getting close, but it was out of my religion to leave such an ugly solution. I was searching the Internet and bumped into Traefik. Integration was surprisingly fast and easy. I thought to myself: \u0026ldquo;That is how modern DevOps should look like\u0026rdquo;. It is been 3 years since I deal with Traefik, and today I wanna tell about one of the most much wished-for feature.\nStep by step setup üêü üêí ü¶ß ‚§¥Ô∏è üë∑üèº error-midl Traefik v2.x comes to us with middlewares and ErrorPage one of them.\nLett\u0026rsquo;s assume:\ntraefik dasboard domain: traefik-board.com your app: my-app.com 0001. Add labels to traefik. Important here traefik.http.routers.traefik.middlewares=maintenance-page@docker. If you try to look for https://service-does-not-exists.com you will get friendly error page.\ntraefik: deploy: labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.services.traefik.loadbalancer.server.port=8080\u0026#34; - \u0026#34;traefik.http.routers.traefik.rule=Host(`${traefik-board.com}`)\u0026#34; - \u0026#34;traefik.http.routers.traefik.entryPoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.traefik.tls=true\u0026#34; - \u0026#34;traefik.http.routers.traefik.middlewares=maintenance-page@docker\u0026#34; 0010. Create dedicated service for error page. Make sure service would be as highly available as Traefik! I recommend to set deploy mode: global and placement: constraints: - node.role == manager. If you don\u0026rsquo;t, hello boring black page again.\nmaintenance-page: deploy: labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.services.maintenance-page.loadbalancer.server.port=80\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.rule=HostRegexp(`{host:.+}`)\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.priority=1\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.middlewares=maintenance-page@docker\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.entrypoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.tls=true\u0026#34; - \u0026#34;traefik.http.middlewares.maintenance-page.errors.status=400-599\u0026#34; - \u0026#34;traefik.http.middlewares.maintenance-page.errors.service=maintenance-page\u0026#34; - \u0026#34;traefik.http.middlewares.maintenance-page.errors.query=/\u0026#34; These lines are essential. You tell the server to catch all requests and redirect them to maintenance-page service. priority=1 means \u0026ldquo;all-catcher\u0026rdquo; is not alone in the system. There is someone higher who wants to handle requests firstly, for example, \u0026ldquo;all-catcher\u0026rdquo; for http-\u0026gt;https redirect.\ntraefik.http.routers.maintenance-page.priority=1 traefik.http.routers.maintenance-page.rule=HostRegexp(`{host:.+}`) 0011. Add priority=100 to app service.\napp: deploy: labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.frontend.rule=Host(`${my-app.com}`)\u0026#34; - \u0026#34;traefik.http.services.frontend.loadbalancer.server.port=80\u0026#34; - \u0026#34;traefik.http.routers.frontend.entrypoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.frontend.tls=true\u0026#34; - \u0026#34;traefik.http.routers.frontend.priority=100\u0026#34; Testing ü§πüèΩ‚Äç‚ôÄÔ∏è case 1: app service goes down or updating replicas: 0:1 -\u0026gt; see maitenance page case 2: user searched for https://service-does-not-exists.com -\u0026gt; see maitenance page Further reading üë©üèΩ‚Äçüíª hot thread about this topic on the Containous Community Forum nginx based error pages So, that‚Äôs all for now üòä. üë©‚ÄçüöÄ was happy to share my experience, and I\u0026rsquo;m sure this solution will save a bunch of time for someone. Happy hacking=) ü¶æ\n","permalink":"https://juliakostrikova.com/posts/cloud/traefik-error-page/","summary":"What you will learn today ü¶¶: what is Traefik and why I decisively recommend to use it with Docker Swarm how to show custom error page if Traefik returns 404 (service not found) How I met Traefik üòÅ Traefik is a top modern reverse proxy and load balancer. I started to use it from major version 1. At that moment, I already had Docker Swarm cluster, which I began to prepare for production use, and only one thing in the coherent picture has been left: \u0026ldquo;External Load Balancer\u0026rdquo;.","title":"Custom error page in Traefik v2.x. Finally"},{"content":"Task description on Codesignal\nI‚Äôve spend decent amount of time in order to figure out that I don‚Äôt need any string manipulation in this task üòÖ. My first try was adding missing zeros to the beginning of number slot, convert int to str and back, then the resulting string split by 4 but no luck ‚Ä¶ It turned out that is simple math üòÄ I wanna share my solution with detailed comments ‚¨áÔ∏è (Time complexity O(n) )\n\u0026lt;!-- Python code --\u0026gt; \u0026#39;\u0026#39;\u0026#39; Test cases a = [123, 4, 5] b = [100, 100, 100] a = [9876, 5432, 1999] b = [1, 8001] a = [1] b = [9999, 9999, 9999, 9999, 9999, 9999] \u0026#39;\u0026#39;\u0026#39; \u0026#39;\u0026#39;\u0026#39; helper \u0026#39;\u0026#39;\u0026#39; class ListNode: def __init__(self, x): self.value = x self.next = None \u0026#39;\u0026#39;\u0026#39; helper \u0026#39;\u0026#39;\u0026#39; def array_to_linked_list(arr): if not arr: return None head = ListNode(arr[0]) root = head for i in range(1, len(arr)): new = ListNode(arr[i]) head.next = new head = new return root \u0026#39;\u0026#39;\u0026#39; helper \u0026#39;\u0026#39;\u0026#39; def linked_to_array(l): res = [] while l is not None: res.append(l.value) l = l.next return res \u0026#39;\u0026#39;\u0026#39; helper \u0026#39;\u0026#39;\u0026#39; def reverse(l): prev = None head = l while head: next = head.next head.next = prev prev = head head = next return prev def addTwoHugeNumbers(a, b): \u0026#39;\u0026#39;\u0026#39; in order to proceed with addition from right to left we need to reverse linked list and when we finish reverse it back \u0026#39;\u0026#39;\u0026#39; a = reverse(a) b = reverse(b) \u0026#39;\u0026#39;\u0026#39; init resulting linked list and buffer \u0026#39;\u0026#39;\u0026#39; head = None root = None buffer = 0 while a or b: \u0026#39;\u0026#39;\u0026#39; set 0 if NodeList element is already None \u0026#39;\u0026#39;\u0026#39; a_value = 0 if not a else a.value b_value = 0 if not b else b.value sum_els = a_value + b_value if buffer != 0: sum_els = sum_els + buffer buffer = 0 \u0026#39;\u0026#39;\u0026#39; 10000 - condition to move to another number order buffer - always 1 because 9 is the highest value in decimal numeral system \u0026#39;\u0026#39;\u0026#39; if sum_els \u0026gt; 9999: sum_els = sum_els - 10000 buffer = 1 new = ListNode(sum_els) if head is None: head = new root = head else: head.next = new head = new a = None if not a else a.next b = None if not b else b.next \u0026#39;\u0026#39;\u0026#39; if we have not empty buffer at the end of addition then add one more NodeList to the beginning \u0026#39;\u0026#39;\u0026#39; if buffer != 0: new = ListNode(buffer) head.next = new return reverse(root) \u0026#39;\u0026#39;\u0026#39; we are going to do column addition like we do at school but with only difference: addition not one by one but four by four for example: 123, 4, 5 100,100,100 \u0026#39;\u0026#39;\u0026#39; a = array_to_linked_list(a) b = array_to_linked_list(b) result = addTwoHugeNumbers(a, b) print(linked_to_array(result)) ","permalink":"https://juliakostrikova.com/posts/algorithms/addtwohugenumbers/","summary":"Task description on Codesignal\nI‚Äôve spend decent amount of time in order to figure out that I don‚Äôt need any string manipulation in this task üòÖ. My first try was adding missing zeros to the beginning of number slot, convert int to str and back, then the resulting string split by 4 but no luck ‚Ä¶ It turned out that is simple math üòÄ I wanna share my solution with detailed comments ‚¨áÔ∏è (Time complexity O(n) )","title":"Solution for CodeSignal: addTwoHugeNumbers (Linked List part üëØ)"},{"content":"1 Add tools specifically to php monitoring in order to answer the questions ‚¨áÔ∏è what kind of php scripts were called most of the time? what kind of php scripts took the most CPU? We choose open-source tool Pinba.\nphp pinba extension sends data over UDP in protobuf format. pinba server accumulates and processes data and send it to data storage in your choice. (In our case it is Clickhouse DB)\nIt is not a secret that the most popular script in php frameworks is index.php but this information is barely helpful. So we need some ‚ÄúCPU statistics per route‚Äù.\n‚ÄúCPU statistics per route‚Äù means that: we have to implement one more chart in Grafana that shows CPU consumption in the matter of requested route, for example, V2\\Community\\Model.php And only after that we can give this info to developers for investigation what kind of code took the most CPU in the route: V2\\Community\\Model.php When the developer got the list of routes it is time to look inside those routes and investigate what cause high CPU. On this stage Blackfire can help. Blackfire will show you CPU Wall Time. To keep things simple, Wall Time is usually split in two main parts: the CPU Time and the I/O Time.\nThe CPU time is the amount of time the CPU was used for processing instructions. The I/O time is the time the CPU waited for input/output (I/O) operations.\nI/O time could be divided into two parts: the network and the disk.\nNetwork activity includes calls to databases like:\nMySQL, PostgreSQL, or MongoDB; HTTP calls to web services and APIs; Calls to cache systems like Redis and Memcached; Communications with services like queues, email daemons, remote filesystems; etc. Disk activity occurs when a program reads files from the filesystem, including loading code or files.\nWe need to distinguish these types of times in order to make further decisions related optimization.\nA CPU-bound program‚Äôs speed depends mostly on the CPU. In other words, CPU utilization is high for long periods of time. The faster the CPU, the faster the code runs. On the contrary, an I/O bound program‚Äôs speed is determined by the time spent waiting for I/O. Faster disks or a faster network improve the overall performance of I/O bound code.\n2 After that, depending on findings ‚¨áÔ∏è IF the reason of high CPU is I/O bound related to DB (too much time waiting for I/O ) then we should consider migrate to RDS and set up storage General Purpose (SSD) or Provisioned IOPS (SSD).\nIF the reason of high CPU : CPU-bound (which is rare case for web applications) then we should consider to optimize algorithms in code.\nto be continued‚Ä¶ üëÄ\n","permalink":"https://juliakostrikova.com/posts/php/high-cpu/","summary":"1 Add tools specifically to php monitoring in order to answer the questions ‚¨áÔ∏è what kind of php scripts were called most of the time? what kind of php scripts took the most CPU? We choose open-source tool Pinba.\nphp pinba extension sends data over UDP in protobuf format. pinba server accumulates and processes data and send it to data storage in your choice. (In our case it is Clickhouse DB)","title":"How to analyse ‚Äúhigh CPU‚Äù in php on production. Part 1"},{"content":"Into üíÜ Pre-conditions: macOS Sierra Docker CE Version 17.06.2-ce-mac27 PHP 7.1.8 XDebug v2.5.0 PhpStorm 2017.2 If you want to set up remote debug from your container you have 2 options:\nxdebug.remote_host = %host_ip% %host_ip% ‚Äî The IP of the machine running your IDE. It‚Äôs assumed on the same host as Docker. xdebug.remote_connect_back = on checks $_SERVER[‚ÄòHTTP_X_FORWARDED_FOR‚Äô] and $_SERVER[‚ÄòREMOTE_ADDR‚Äô] variables to find out which IP address to use. You cannot use xdebug.remote_connect_back because it takes containers‚Äôs internal IP. So xdebug.remote_host is the only option we have. It‚Äôs pretty simple to get host IP on Linux OS just look at : ifconfig \u0026gt; docker0. Moreover you can create a fixed set of IPs for both host and containers using docker network. For instance:\ndocker network create -d bridge \\ ‚Äî subnet 192.168.0.0/24 \\ ‚Äî gateway 192.168.0.1 \\ dockernet Now each container can connect to the host under the fixed IP 192.168.0.1 But the docker0 interface is not available for Docker in macOS. This interface is actually within HyperKit. So unfortunately there is no a reliable way of looping back to host for macOS. The only solution is to create your local IP‚Äôs alias to something else : sudo ifconfig lo0 alias 10.254.254.254\nHowTo guide üë©‚Äçüíª sudo ifconfig lo0 alias 10.254.254.254 (optional) For permanent setting up save.plist file sudo curl -o /Library/LaunchDaemons/com.yuklia.docker_localhost_alias.plist build docker containers \u0026hellip; add server to IDE storm1 configure Debug‚Äôs port in my case it‚Äôs 9002. xDebug client will knock to this port. storm2 configure DBGp Proxy storm3 Bonus üç∞ If you need to get a shell of the VM that was created by HyperKit run:\nscreen ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/tty or screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty For getting all screens run: screen -ls For exiting screen run : screen -X -S %screen_name% kill Once you get the tty running you can navigate to /var/lib/docker: storm4 Here is my Dockerfile and docker-compose versions for php:7-fpm with XDebug So that‚Äôs all for now üòä. Hope it will save a bunch of time for someone! I will be glad to get comments and questions. Happy hacking=) ü¶æ\n","permalink":"https://juliakostrikova.com/posts/php/xdebug-macos/","summary":"Into üíÜ Pre-conditions: macOS Sierra Docker CE Version 17.06.2-ce-mac27 PHP 7.1.8 XDebug v2.5.0 PhpStorm 2017.2 If you want to set up remote debug from your container you have 2 options:\nxdebug.remote_host = %host_ip% %host_ip% ‚Äî The IP of the machine running your IDE. It‚Äôs assumed on the same host as Docker. xdebug.remote_connect_back = on checks $_SERVER[‚ÄòHTTP_X_FORWARDED_FOR‚Äô] and $_SERVER[‚ÄòREMOTE_ADDR‚Äô] variables to find out which IP address to use. You cannot use xdebug.","title":"XDebug for PHP docker container on macOS"},{"content":"","permalink":"https://juliakostrikova.com/posts/misc/auto-rollout-terraform/","summary":"","title":""}]
[{"content":"Disclaimer Meet Toba the cat, the latest security shield model, equipped with 24/7 surveillance mode and an aggressive defense mechanism when unauthorized movement is detected. üö®üòº\nAny attempt to relocate this elite guardian from the server rack results in immediate resistance, accompanied by sharp glares and indignant meows. üõëüêæ\nPlan to Host Mastodon on 4 √ó Raspberry Pis In my current homelab project, I am building a bare-metal Kubernetes cluster to host ~10‚Äì50 containers per node. What app will run there? Well, I‚Äôll figure that out along the way, but I‚Äôm considering hosting Mastodon. (For those who don‚Äôt know, Mastodon is a decentralized microblogging platform.)\nNo Hypervisors for This Project üö´ As you might know from my previous posts, I have 4√ó Raspberry Pi 5 boards. Each features a 64-bit quad-core ARM Cortex-A76 processor running at 2.4GHz with 8 GB RAM. In my setup, 3 Raspberry Pis act as worker nodes, while 1 serves as the control plane (in the future, I‚Äôll increase it to 3 to meet RAFT consensus, but for now, this setup is fine).\nKubernetes pods will run directly on the host OS. The OS is Raspberry Pi OS (64-bit), booted over the LAN network. If you‚Äôre interested, you can read more in this post about setting up PXE boot.\nEven though Raspberry Pi 5 delivers a 2‚Äì3√ó increase in CPU performance compared to Raspberry Pi 4, I‚Äôm avoiding virtualization solutions like VMware and Proxmox (I like Proxmox, but not for this setup) because they introduce unnecessary overhead.\n‚ûï One day, I‚Äôm thinking about building a voice assistant (currently, I have a Siri HomePod but want to make it smarter) that interacts with LLMs and controls physical devices like smart sockets, lights, or energy monitoring systems. For that, I need access to GPIO pins (direct hardware interaction). Virtualization makes passing through \u0026ldquo;exotic\u0026rdquo; hardware problematic, so running on bare metal is the better choice.\nWhat is for dinner? üí≠ In today\u0026rsquo;s post, I am going to shed a light on the issue I faced with PXE boot on NFS and containderd\u0026rsquo;s component snapshotter that is not compatible with NFS. But how I come to that conclusion? let\u0026rsquo;s check it out.\ndmesg, linux kernel dump tool spaws this log ‚¨áÔ∏è\n[Thu Feb 6 09:20:56 2025] overlayfs: upper fs does not support tmpfile. [Thu Feb 6 09:20:56 2025] overlayfs: upper fs does not support RENAME_WHITEOUT. [Thu Feb 6 09:20:56 2025] overlayfs: failed to set xattr on upper [Thu Feb 6 09:20:56 2025] overlayfs: ...falling back to redirect_dir=nofollow. [Thu Feb 6 09:20:56 2025] overlayfs: ...falling back to uuid=null. [Thu Feb 6 09:20:56 2025] overlayfs: upper fs missing required feature and kubectl describe pod shows this error, but they are both about that same matter.\nkubectl describe pod -l app=nginx --namespace default --kubeconfig /opt/hl-control-node/_tmp/kubeconfigs/admin.kubeconfig Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 13m (x19065 over 2d23h) kubelet (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox \u0026#34;1323c487a948330ef32a782dc48095e74998758163f49401ce91cd649013d8ec\u0026#34;: failed to create containerd task: failed to create shim task: failed to mount rootfs component: invalid argument Warning FailedCreatePodSandBox 5m29s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start s overlayfs is one of the file systems that used by snapshotter. upper fs is literally my \u0026ldquo;upper FS\u0026rdquo; in my case it is NFS, since my Entire Root Filesystem (/) is on NFS.\ndf -h Filesystem Size Used Avail Use% Mounted on udev 3.8G 0 3.8G 0% /dev tmpfs 806M 5.3M 801M 1% /run 192.168.101.253:/volume1/RPi5-PXE/node4/rootfs 885G 148G 737G 17% / tmpfs 4.0G 0 4.0G 0% /dev/shm tmpfs 5.0M 48K 5.0M 1% /run/lock 192.168.101.253:/volume1/RPi5-PXE/node4 885G 148G 737G 17% /boot tmpfs 806M 0 806M 0% /run/user/1000 all this means that network file system doesn\u0026rsquo;t have required features like: tmtfile; RENAME_WHITEOUT; xattr.\ncontainerd architectureImage taken from official source https://containerd.io\nmy first thought was to switch overlayfs into btrsfs or devmapper storage drivers, but is is \u0026ldquo;bargain one trouble for another\u0026rdquo; rather then help. all these storage drivers works by allocating block storage.\nhere is my illustration how block storage works: block storage in comparision to network file storage, it has a logic on top of file storage that is responsible for assign unique identifiers to files which are stored in the data lookup table. read operations are obviously much more faster then in a unsturctured storage like nfs.\nand it means that for making k8s work in the current setup i need to \u0026ldquo;transform\u0026rdquo; those directories that kuberenets need into block storage type. and solution for that is iSCSI (/a…™Ààsk åzi/ eye-SKUZ-ee). boom.\niSCSIfication iSCSI (Internet Small Computer System Interface)\nWiki: iSCSI provides block-level access to storage devices by carrying SCSI commands over a TCP/IP network.\niSCSI allows remote data sharing (same as nfs) but at the block level. It enables data exchange between multiple client machines and a block storage device (or block server), which is accessed similarly to a local disk drive.\nhere is illustration how it works in with my homelab devices\niSCSI client/server auth flow Setting up iSCSI on Synology NAS create an iSCSI Targets for each cluster node. define logical unit numbers (LUN) for: /var/lib/containerd (stores container images and metadata) /var/lib/kubelet (kubernetes node-specific data) /var/logs/pods (stores logs from running containers) Configuring iSCSI Initiator on Raspberry PI nodes install the iscsi client: sudo apt install -y open-iscsi sudo systemctl enable --now open-iscsi discover the iscsi targets: iscsiadm -m discovery -t st -p 192.168.101.253 login to the iscsi target: sudo iscsiadm -m node --targetname \u0026#34;iqn.2000-01.com.synology:nas.Target-node3\u0026#34; --portal \u0026#34;192.168.101.253:3260\u0026#34; --login 3.1. enable iSCSI auto-login, to restore session after reboot\nsudo iscsiadm -m node --targetname \u0026#34;iqn.2000-01.com.synology:nas.Target-node3\u0026#34; --portal \u0026#34;192.168.101.253:3260\u0026#34; --op update -n node.startup -v automatic format the block devices as ext4: sudo mkfs.ext4 -L kubelet /dev/sda sudo mkfs.ext4 -L logs /dev/sdb sudo mkfs.ext4 -L containerd /dev/sdc yuklia@node3:~ $ sudo blkid /dev/sda /dev/sda: LABEL=\u0026#34;kubelet\u0026#34; UUID=\u0026#34;317346de-f632-4f5d-9ec0-90770f56938d\u0026#34; BLOCK_SIZE=\u0026#34;4096\u0026#34; TYPE=\u0026#34;ext4\u0026#34; yuklia@node3:~ $ sudo blkid /dev/sdb /dev/sdb: LABEL=\u0026#34;logs\u0026#34; UUID=\u0026#34;d35111a5-f56a-4bde-a731-87e630fa0aed\u0026#34; BLOCK_SIZE=\u0026#34;4096\u0026#34; TYPE=\u0026#34;ext4\u0026#34; yuklia@node3:~ $ sudo blkid /dev/sdc /dev/sdc: LABEL=\u0026#34;containerd\u0026#34; UUID=\u0026#34;8c83f195-eb3e-4226-914d-98837e3ddcca\u0026#34; BLOCK_SIZE=\u0026#34;4096\u0026#34; TYPE=\u0026#34;ext4\u0026#34; 4.1 update fstab to persist mount targets after reboot\ncat /etc/fstab proc /proc proc defaults 0 0 192.168.101.253:/volume1/RPi5-PXE/node4 /boot nfs defaults,vers=3,proto=tcp 0 0 UUID=317346de-f632-4f5d-9ec0-90770f56938d /var/lib/kubelet ext4 defaults,_netdev 0 0 UUID=d35111a5-f56a-4bde-a731-87e630fa0aed /var/logs/pods ext4 defaults,_netdev 0 0 UUID=8c83f195-eb3e-4226-914d-98837e3ddcca /var/lib/containerd ext4 defaults,_netdev 0 0 mount the block devices:\nsudo mount /dev/sda /var/lib/kubelet sudo mount /dev/sdb /var/logs/pods sudo mount /dev/sdc /var/lib/containerd update /etc/fstab to persist mounts:\nUUID=317346de-f632-4f5d-9ec0-90770f56938d /var/lib/kubelet ext4 defaults,_netdev 0 0 UUID=d35111a5-f56a-4bde-a731-87e630fa0aed /var/logs/pods ext4 defaults,_netdev 0 0 UUID=8c83f195-eb3e-4226-914d-98837e3ddcca /var/lib/containerd ext4 defaults,_netdev 0 0 job done\nyuklia@node3:~ $ df -h Filesystem Size Used Avail Use% Mounted on udev 3.8G 0 3.8G 0% /dev tmpfs 806M 5.4M 800M 1% /run 192.168.101.253:/volume1/RPi5-PXE/node4/rootfs 885G 227G 658G 26% / tmpfs 4.0G 0 4.0G 0% /dev/shm tmpfs 5.0M 48K 5.0M 1% /run/lock /dev/sdb 4.9G 24K 4.6G 1% /var/logs/pods /dev/sda 11G 124K 11G 1% /var/lib/kubelet /dev/sdc 9.8G 270M 9.0G 3% /var/lib/containerd 192.168.101.253:/volume1/RPi5-PXE/node4 885G 227G 658G 26% /boot tmpfs 806M 0 806M 0% /run/user/1000 Smoke test ? kubectl get pod -l app=nginx\noh, no CrashLoopBackOff ... Reason: CrashLoopBackOff Last State: Terminated Reason: StartError Message: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: unified resource \u0026#34;memory.oom.group\u0026#34; can\u0026#39;t be set: controller \u0026#34;memory\u0026#34; not available Root Cause: memory.oom.group Can\u0026rsquo;t Be Set This means\nthe container runtime (containerd + runc) is trying to set memory cgroup configurations, but the memory cgroup controller is missing or not enabled on your system.\nupdating kernel boot parameters with systemd.unified_cgroup_hierarchy=1 cgroup_enable=memory helped.\nsudo cat /boot/cmdline.txt dwcotg.lpm_enable=0 console=serial0,115200 console=tty1 elevator=deadline rootwait rw root=/dev/nfs nfsroot=192.168.101.253:/volume1/RPi5-PXE/node4/rootfs,v3,tcp ip=dhcp cgroup_enable=memory cgroup_memory=1 systemd.unified_cgroup_hierarchy=1 Smoke test-test üß™ check that all nodes up \u0026amp; running\nkubectl get nodes --kubeconfig /opt/hl-control-node/_tmp/kubeconfigs/admin.kubeconfig NAME STATUS ROLES AGE VERSION node1 Ready \u0026lt;none\u0026gt; 37d v1.31.2 node2 Ready \u0026lt;none\u0026gt; 37d v1.31.2 node3 Ready \u0026lt;none\u0026gt; 37d v1.31.2 check that nginx pod up \u0026amp; running\nkubectl get pod -l app=nginx --namespace default --kubeconfig /opt/hl-control-node/_tmp/kubeconfigs/admin.kubeconfig NAME READY STATUS RESTARTS AGE nginx-54f87867d6-9tbgl 1/1 Running 0 3h13m Final Thoughts I started with PXE boot and NFS, thinking it would be a clean and efficient solution. However, I quickly encountered limitations due to OverlayFS incompatibility in containerd. After troubleshooting, I realized that NFS wasn‚Äôt sufficient and decided to pivot to iSCSI block storage.\nSetting up iSCSI on my Synology NAS and configuring the initiators on the Raspberry Pi nodes required some effort. Once completed, it resolved the persistent storage issue. Of course, Kubernetes wouldn‚Äôt let me off that easy. I hit another roadblock with memory cgroup issues, which I fixed by tweaking kernel boot parameters.\nAfter all the adjustments and smoke tests, the cluster is now running smoothly. I can finally deploy workloads without storage headaches. Next step? Automating iSCSI provisioning with OpenTofu (Terraform) to make scaling effortless. Stay tuned for that adventure! üöÄ\n","permalink":"https://juliakostrikova.com/posts/homelab/iscsi_pxe_mount/","summary":"Disclaimer Meet Toba the cat, the latest security shield model, equipped with 24/7 surveillance mode and an aggressive defense mechanism when unauthorized movement is detected. üö®üòº\nAny attempt to relocate this elite guardian from the server rack results in immediate resistance, accompanied by sharp glares and indignant meows. üõëüêæ\nPlan to Host Mastodon on 4 √ó Raspberry Pis In my current homelab project, I am building a bare-metal Kubernetes cluster to host ~10‚Äì50 containers per node.","title":"k8s cluster on Raspberry PI's with iSCSI block storage"},{"content":"introduction I\u0026rsquo;ve always been intrigued by the idea of creating a home data center with Raspberry PIs as a computable instances. However, managing individuals node with SD cards is not fun üòº. So decided to try PXE (Preboot Execution Environment) booting. Instead of dealing with each SD card individually, PXE protocol lets me manage all my RPis from one central server, making updates and further patches easier. In this post, I\u0026rsquo;ll share how you can set up PXE booting for your RPi fleet and enjoy these benefits to :)\nmy appliances I;ll provide your with examples that are specific to my devices but once you would grasp the idea you could to the same on your set up.\n1 x UDM Dream Machine SE 1 x Synology NAS 4 x RaspberryPI 5 4 x PoE Hat for RaspberryPI 5 1 x microSD card (optional) 1 x monitor + HDMI to mini HDMI cable plan After a series of trials and errors, I found out the recipe that worked for me. I am happy to share it with you. :)\n[UDM] create a separate subnet (e.g VLAN ) for RPi\u0026rsquo;s and NAS [UDM] configure DHCP server [SD card] Write OS [RPi] update EEPROM firmware for all RPi machines [NAS] enable NFS service [NAS] create SharedFolder [NAS] enable TFTP service [NAS] enable rsync [NAS] create folders node1/rootfs in TFTP root folder [RPi node1] mount NFS shared folder /volume1/RPI5-PXE/node1/rootfs to node1 [RPI node1] upload / root filesystem to [NAS] /volume1/RPI5-PXE/node1/rootfs [NAS] copy the content of /volume1/RPI5-PXE/node1/rootfs/boot/firmware into the /volume1/RPI5-PXE/node1/ [NAS] modify cmdline.txt in /volume1/RPI5-PXE/node1/ [NAS] modify fstab in /volume1/RPI5-PXE/node1/rootfs/etc/ repeat steps from 10 - 14 for other nodes{2,3,4,..n} final step 1 [UDM] create a separate subnet (e.g VLAN ) for RPi\u0026rsquo;s and NAS Network segmentation gives me better devices organization / isolation, security benefits (firewall rules, content filtering, etc). For now, in my home network, I have 2 subnets: default and cluster. default includes all devices that connected to home mesh, including phones, PlayStations, IoT devices, ect. And cluster is a dedicated segment with network devices dedicated to my home data center project. Our main focus will be concentrated in cluster subnet today üéØ. Assign cluster subent to specific ports. Bottom row is RPi\u0026rsquo;s ports. In the top row, the port on the far right belongs to the NAS. ports assignment 2 [UDM] configure DHCP server DHCP server plays is big role in PXE envirounment. And it requires extra configuraions because it not only provide IP address, IP mask, etc to all booting DHCP clients but also provides TFTP server IP address and name of NBP only to identified booting clients. Practicly speaking, you need to configure DHCP Options which iusually is and advanced section in your DHCP settings.\nAdjust your DHCP Options to these values:\ncode value 60 PXEClient 66 192.168.101.253 67 bootcode.bin 43 1.2.3.4 43 - Vendor Specific Information\n60 - Vendor Class Identifier\n66 - TFTP Server Name\n67 - Bootfile Name\nI would like to point your attention to option 43. In the official RPi tech spec it was told:\nVendor-Option Option 43 contains the important part of the reply. This must contain the string \u0026ldquo;Raspberry Pi Boot\u0026rdquo;. Due to a bug in the boot ROM, you may need to add three spaces to the end of the string.\nIt might work in your case, but in my no. This field on my router has a specific validation rule and it accepts only IP address format. So I set 1.2.3.4\n3 [SD card] Write OS micro SD card You can install any OS using Raspberry Pi Imager. My choice is Raspberry PI OS (Lite) 64-bit. A port of Debian Bookworm with no desktop environment.\nEnable SSH agent 4 [RPi] update EEPROM firmware for all RPi machines Install SD card into RPi node1. Connect to node1 via SSH. Run to see your current EEPROM settings.\nvcgencmd bootloader_config Run command below to extract settings into the file bootconf.txt\ncp /lib/firmware/raspberrypi/bootloader-2712/stable/pieeprom-2024-06-05.bin pieeprom.bin rpi-eeprom-config pieeprom.bin \u0026gt; bootconf.txt Modify bootconf.txt :\n[all] BOOT_UART=1 POWER_OFF_ON_HALT=0 BOOT_ORDER=0x21 TFTP_PREFIX=1 TFTP_PREFIX_STR=node1/ PXE_OPTION43=1.2.3.4 Apply new EEPROM config.\nrpi-eeprom-config --out pieeprom-new.bin --config bootconf.txt pieeprom.bin rpi-eeprom-update -d -f ./pieeprom-new.bin reboot Do the same steps for all machines. But change TFTP_PREFIX_STR=node{2,3,4}/\n5 [NAS] enable NFS service 6 [NAS] create SharedFolder 7 [NAS] enable TFTP service 8 [NAS] enable rsync 9 [NAS] create folders node1/rootfs in TFTP root folder SSH to your NAS and create subfolders for each node\ndrwxrwxrwx+ 1 root root 52 Jul 25 18:31 . drwxr-xr-x 1 root root 680 Jul 25 17:41 .. drwxrwxrwx+ 1 yuklia users 1280 Jul 27 23:12 node1 drwxrwxrwx+ 1 yuklia users 1232 Jul 25 18:11 node2 drwxrwxrwx+ 1 yuklia users 1232 Jul 25 18:19 node3 drwxrwxrwx+ 1 yuklia users 1232 Jul 25 18:39 node4 yuklia@nas:/volume1/RPi5-PXE$ 10 [RPi node1] mount NFS shared folder /volume1/RPI5-PXE/node1/rootfs to node1 sudo mkdir -p /mnt/img sudo mount -t nfs 192.168.101.253:/volume1/RPI5-PXE/node1/rootfs /mnt/img 11 [RPI node1] upload / root filesystem to [NAS] /volume1/RPI5-PXE/node1/rootfs sudo rsync -a --exclude={\u0026#34;/proc/*\u0026#34;,\u0026#34;/sys/*\u0026#34;,\u0026#34;/dev/*\u0026#34;,\u0026#34;/run/*\u0026#34;,\u0026#34;/tmp/*\u0026#34;,\u0026#34;/mnt/*\u0026#34;} --info=progress2 / /mnt/img 12 [NAS] copy the content of /volume1/RPI5-PXE/node1/rootfs/boot/firmware into the /volume1/RPI5-PXE/node1/ cp -r /volume1/RPI5-PXE/node1/rootfs/boot/firmware/* /volume1/RPI5-PXE/node1/ 13 [NAS] modify cmdline.txt in /volume1/RPI5-PXE/node1/ dwcotg.lpm_enable=0 console=serial0,115200 console=tty1 elevator=deadline rootwait rw root=/dev/nfs nfsroot=192.168.101.253:/volume1/RPi5-PXE/node1/rootfs,v3,tcp ip=dhcp 14 [NAS] modify fstab in /volume1/RPI5-PXE/node1/rootfs/etc/ proc /proc proc defaults 0 0 192.168.101.253:/volume1/RPi5-PXE/node1 /boot nfs defaults,vers=3,proto=tcp 0 0 15 repeat steps from 10 - 14 for other nodes{2,3,4,..n} 16 final step Remove micro SD card, power on PRi\u0026rsquo;s üîå. DHCP server should navigate them to the bootfile location.\nRun findmnt for validation\nyuklia@node-1:~ $ findmnt TARGET SOURCE FSTYPE OPTIONS / 192.168.101.253:/volume1/RPi5-PXE/node1/rootfs nfs rw,relatime,vers=3,rsize=131072,wsize=131072,na |-/sys sysfs sysfs rw,nosuid,nodev,noexec,relatime | |-/sys/kernel/security securityfs securityf rw,nosuid,nodev,noexec,relatime | |-/sys/fs/cgroup cgroup2 cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate,memo | |-/sys/fs/pstore pstore pstore rw,nosuid,nodev,noexec,relatime | |-/sys/fs/bpf bpf bpf rw,nosuid,nodev,noexec,relatime,mode=700 | |-/sys/kernel/debug debugfs debugfs rw,nosuid,nodev,noexec,relatime | |-/sys/kernel/tracing tracefs tracefs rw,nosuid,nodev,noexec,relatime | |-/sys/kernel/config configfs configfs rw,nosuid,nodev,noexec,relatime | `-/sys/fs/fuse/connections fusectl fusectl rw,nosuid,nodev,noexec,relatime |-/proc proc proc rw,relatime | `-/proc/sys/fs/binfmt_misc systemd-1 autofs rw,relatime,fd=30,pgrp=1,timeout=0,minproto=5,m | `-/proc/sys/fs/binfmt_misc binfmt_misc binfmt_mi rw,nosuid,nodev,noexec,relatime |-/dev udev devtmpfs rw,nosuid,relatime,size=3950816k,nr_inodes=2469 | |-/dev/pts devpts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmo | |-/dev/shm tmpfs tmpfs rw,nosuid,nodev | `-/dev/mqueue mqueue mqueue rw,nosuid,nodev,noexec,relatime |-/run tmpfs tmpfs rw,nosuid,nodev,noexec,relatime,size=824576k,mo | |-/run/lock tmpfs tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k | |-/run/credentials/systemd-sysusers.service | | ramfs ramfs ro,nosuid,nodev,noexec,relatime,mode=700 | |-/run/credentials/systemd-sysctl.service | | ramfs ramfs ro,nosuid,nodev,noexec,relatime,mode=700 | |-/run/credentials/systemd-tmpfiles-setup-dev.service | | ramfs ramfs ro,nosuid,nodev,noexec,relatime,mode=700 | |-/run/rpc_pipefs sunrpc rpc_pipef rw,relatime | |-/run/credentials/systemd-tmpfiles-setup.service | | ramfs ramfs ro,nosuid,nodev,noexec,relatime,mode=700 | `-/run/user/1000 tmpfs tmpfs rw,nosuid,nodev,relatime,size=824560k,nr_inodes `-/boot 192.168.101.253:/volume1/RPi5-PXE/node1 nfs rw,relatime,vers=3,rsize=131072,wsize=131072,na This is how my home server rack looks like ü•∞\nInner side of the server rack\nsummary PXE boot up \u0026amp; running and it means that it is a time to host something in my cluster ü§î As a next step I want to set up Kubernetes \u0026ldquo;the hard-way\u0026rdquo;. Stay tuned üòÑ\nThat is the end of the journey. :) I hope it might be handy for your own setup! If you have any questions, don‚Äôt hesitate to write in the comments or DM me on Linkedin. I am always passionate about discussing geeky stuff. üòâ\n","permalink":"https://juliakostrikova.com/posts/homelab/pxe-boot/","summary":"introduction I\u0026rsquo;ve always been intrigued by the idea of creating a home data center with Raspberry PIs as a computable instances. However, managing individuals node with SD cards is not fun üòº. So decided to try PXE (Preboot Execution Environment) booting. Instead of dealing with each SD card individually, PXE protocol lets me manage all my RPis from one central server, making updates and further patches easier. In this post, I\u0026rsquo;ll share how you can set up PXE booting for your RPi fleet and enjoy these benefits to :)","title":"Detailed HowTo: PXE boot multiple RPi (Birthday Edition üç∞)"},{"content":" UniFi Dream Machine In this blog post, I\u0026rsquo;ll guide you through the process of setting up a DHCP server on your Synology NAS within a subnet created on a UniFi Dream Machine Special Edition (UDM SE).\nSynology NAS Prerequisites Before we start, ensure you have the following:\nUDM SE: UniFi Dream Machine Special Edition. Synology NAS: Network Attached Storage device from Synology. Raspberry Pi: Any model will suffice. Ethernet cables: To connect your devices. Step-by-Step Guide 1. Create a Subnet on UDM SE Log in to your UniFi Router: Access your UDM SE via the UniFi Network application.\nCreate a New VLAN:\nGo to Settings \u0026gt; Networks \u0026gt; Create New Network. Name your network (e.g., VLAN101). Set the VLAN ID (e.g., 101). Set the Gateway/Subnet to 192.168.101.1/24. Set DHCP Mode to None. Assign Ports to VLAN:\nGo to Settings \u0026gt; Ports. Assign Port 2 and Port 7 to the newly created VLAN 101. (example) 2. Configure Static IP on Synology NAS Connect Your NAS:\nPlug your Synology NAS into Port 7 of your UDM SE. Log in to Your Synology NAS:\nOpen a web browser and log in to the Synology DSM interface. Install DHCP Server Application:\nGo to Package Center. Search for and install the DHCP Server application. Set Static IP Address:\nGo to Control Panel \u0026gt; Network \u0026gt; Network Interface. Select your LAN connection or both if you have connected both Ethernet ports for redundancy. Click on Edit and set a static IP address within your subnet (e.g., 192.168.101.252). 3. Configure DHCP Server on Synology NAS Open DHCP Server Application:\nAfter installation, open the DHCP Server application from the main menu. Enable DHCP Server:\nGo to the DHCP Server application and enable the server. Set the Primary DNS and Secondary DNS (e.g., 8.8.8.8). Create Subnet List: Add a new subnet with the following details: Network Interface: Select the interface connected to VLAN101. Subnet: 192.168.101.0/24. Start IP: 192.168.101.100. End IP: 192.168.101.200. Subnet Mask: 255.255.255.0. Gateway: 192.168.101.1. 4. Connect and Verify Raspberry Pi Connect Raspberry Pi:\nPlug your Raspberry Pi into Port 2 of your UDM SE. Check DHCP Client Logs:\nOn your Synology NAS, go to the DHCP Server application. Check the Logs table. You should see the Raspberry Pi listed as a new DHCP client with an IP address assigned from the 192.168.101.x range. Conclusion Setting up a DHCP server on your Synology NAS within a UDM SE subnet is a straightforward process that offers robust network management capabilities. By following these steps, you can ensure your devices get IP addresses dynamically while maintaining control over your network configuration. If you encounter any issues, ensure all settings are correctly applied and consult the Synology and UniFi documentation for further troubleshooting.\nHappy networking!\n","permalink":"https://juliakostrikova.com/posts/homelab/dhcp-nas/","summary":"UniFi Dream Machine In this blog post, I\u0026rsquo;ll guide you through the process of setting up a DHCP server on your Synology NAS within a subnet created on a UniFi Dream Machine Special Edition (UDM SE).\nSynology NAS Prerequisites Before we start, ensure you have the following:\nUDM SE: UniFi Dream Machine Special Edition. Synology NAS: Network Attached Storage device from Synology. Raspberry Pi: Any model will suffice. Ethernet cables: To connect your devices.","title":"Setting Up a DHCP Server on Synology NAS in a UniFi Dream Machine SE Subnet"},{"content":"RPI 5 on Ubuntu 24.04 (headless): Connect to WiFi / Mobile Hotspot Introduction In my free time, I enjoy tinkering with my home lab, exploring new projects and challenges. Recently, I decided to connect my Raspberry Pi 5 to a mobile hotspot‚Äîa seemingly straightforward task. However, it turned out not so smooth. In this post, I\u0026rsquo;ll walk you through how I resolved the error brcmfmac: brcmf_set_channel: set chanspec fail, reason -52 and successfully connected Raspberry Pi to a mobile hotspot.\nPrerequisites Raspberry Pi 5 Ubuntu 24.04 (headless) Mobile phone with hotspot enabled Access to a terminal or SSH connection Mobile hotspot credentials (SSID and password) Step 1: Create wpa_supplicant.conf Create and configure the wpa_supplicant configuration file to connect to your mobile hotspot.\nvi /etc/wpa_supplicant/wpa_supplicant.conf Add the following content, replacing YourHotspotSSID, YourHotspotPassword, YourContryCode with your actual hotspot credentials.\ncountry=YourContryCode ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\u0026#34;YourHotspotSSID\u0026#34; psk=\u0026#34;YourHotspotPassword\u0026#34; key_mgmt=WPA-PSK } Step 2: Configure Netplan Netplan is used to configure networking on Linux. Create a netplan configuration file.\n‚ÑπÔ∏è netplan Netplan is a utility for easily configuring networking on a linux system. You simply create a YAML description of the required network interfaces and what each should be configured to do. From this description Netplan will generate all the necessary configuration for your chosen renderer tool.\nNetplan reads network configuration from /etc/netplan/*.yaml which are written by administrators, installers, cloud image instantiations, or other OS deployments. During early boot, Netplan generates backend specific configuration files in /run to hand off control of devices to a particular networking daemon.\nNetplan currently works with these supported renderers: NetworkManager Systemd-networkd\nvi /etc/netplan/01-iphone-hotspot-ap.yaml Add the following configuration, substituting wlan0 with your wireless interface name.\nnetwork: version: 2 renderer: networkd wifis: wlan0: dhcp4: yes dhcp6: yes access-points: \u0026#34;YourHotspotSSID\u0026#34;: password: \u0026#34;YourHotspotPassword\u0026#34; Apply the netplan configuration.\nnetplan generate netplan apply --debug Step 3: Restart wpa_supplicant Restart the wpa_supplicant service to apply the new configuration.\nsystemctl restart wpa_supplicant Step 4: Check Connection Status Verify that your device is connected to the mobile hotspot using netplan status.\nnetplan status or ip link show | grep wlan0 üßØ Troubleshooting: Resolving brcmfmac: brcmf_set_channel: set chanspec fail, reason -52 If you encounter the error brcmfmac: brcmf_set_channel: set chanspec fail, reason -52, it may indicate issues with the wireless regulatory domain settings or interference from USB 3.0 devices.\nCheck Regulatory Domain:\nEnsure the regulatory domain is set correctly.\niw reg get iw reg set US # Replace US with your country code To make this persistent, edit the CRDA configuration file.\nvi /etc/default/crda Add or modify the following line:\nREGDOMAIN=US Update Firmware and Drivers:\nEnsure you have the latest firmware and drivers.\napt update apt install --reinstall linux-firmware reboot Conclusion Connecting a headless Ubuntu 24.04 system to a mobile hotspot involves configuring wpa_supplicant and netplan correctly. By following the steps outlined above, you can establish a stable Wi-Fi connection, even in the face of potential issues like the brcmfmac: brcmf_set_channel: set chanspec fail, reason -52 error. Properly setting the regulatory domain and ensuring up-to-date firmware are key to resolving such problems.\nissues to follow:\nhttps://github.com/raspberrypi/linux/issues/6049 https://lore.kernel.org/all/99977c876429f33d8dbab18d7c3e71590585263b.camel@sipsolutions.net/T/ https://forums.raspberrypi.com/viewtopic.php?t=367466 https://github.com/home-assistant/operating-system/issues/3367\nnetplan code spippets:\nhttps://github.com/canonical/netplan/tree/main/examples\nHappy serfing üèÑ :)\nnetplan_status ","permalink":"https://juliakostrikova.com/posts/homelab/mobile-hotspot-on-rpi5/","summary":"RPI 5 on Ubuntu 24.04 (headless): Connect to WiFi / Mobile Hotspot Introduction In my free time, I enjoy tinkering with my home lab, exploring new projects and challenges. Recently, I decided to connect my Raspberry Pi 5 to a mobile hotspot‚Äîa seemingly straightforward task. However, it turned out not so smooth. In this post, I\u0026rsquo;ll walk you through how I resolved the error brcmfmac: brcmf_set_channel: set chanspec fail, reason -52 and successfully connected Raspberry Pi to a mobile hotspot.","title":"RPI 5 on Ubuntu 24.04 (Headless): Connect to WiFi / Mobile Hotspot"},{"content":"Introduction UML diagrams play a crucial role in software engineering for visualizing and structuring information. This article explores the seamless automation of UML diagram creation using ChatGPT (LLM) and the PlantUML tool. The aim is to provide a valuable resource for Solutions Architects and engineers looking to efficiently communicate and pitch ideas through visual representations.\nIn this tutorial, we\u0026rsquo;ll focus on visualization of the high-level system design of Netflix. By the end of this article, you\u0026rsquo;ll have a clear understanding of how ChatGPT and PlantUML can be harnessed to expedite the visualization of systems design while saving time on learning PlatUML syntax.\nStep 1: Gather Requirements In Scope Identification and representation of key Netflix components, such as:\nContent Ingestion Workflow Content Replication Mechanism Content Delivery Networks (CDNs) Data Management Layer Playback Service Steering Service Client Devices Step 2: Compose a Prompt for ChatGPT Prompt Engineer: Create a UML sequence diagram using PlantUML to represent a given algorithm. The output should be generated using the PlantUML language with the *wsd syntax. Here is an algorithm.\n1. Creators kick off the process to upload videos onto the system. 2. The videos get spread out to various networks, aiming to reach users through CDN magic. 3. CDNs shoot back info to the system, covering health checks, file inventory, and the smartest routes for data travel. 4. Video details, along with CDN info, find a cozy spot in the data storage. 5. Users on their devices ask to play stuff, starting a cool workflow for different titles, like movies or shows. 6. The Playback service figures out which bits and pieces are needed for playing the chosen title. 7. The Steering service, using some smart moves, picks the best CDNs for grabbing the necessary files. It then cooks up some fancy URLs and sends them to the user\u0026#39;s device. 8. The user\u0026#39;s device sends out requests to the chosen CDNs, starting the process of grabbing the files. CDNs step up and serve the files requested, making sure everything lands on the user\u0026#39;s device for a good time. Step 3: The *wsd output from ChatGPT @startuml actor Creators actor CDNs actor DataStorage actor Users actor PlaybackService actor SteeringService actor UserDevice note left of Creators Creators kick off the process to upload videos onto the system. end note Creators -\u0026gt; DataStorage: Upload Videos activate DataStorage note left of Creators Videos spread out to various networks, aiming to reach users through CDN magic. end note DataStorage -\u0026gt; CDNs: Distribute Videos activate CDNs CDNs --\u0026gt; DataStorage: Report Info (Health, Inventory, Routes) deactivate CDNs note left of DataStorage Video details and CDN info find a cozy spot in the data storage. end note DataStorage -\u0026gt; CDNs: Persist Video Details and CDN Info note left of Users Users on their devices ask to play stuff, starting a cool workflow for different titles, like movies or shows. end note Users -\u0026gt; PlaybackService: Request to Play activate PlaybackService note left of PlaybackService Playback service figures out which bits and pieces are needed for playing the chosen title. end note PlaybackService -\u0026gt; SteeringService: Determine Components activate SteeringService note left of SteeringService Steering service, using some smart moves, picks the best CDNs for grabbing the necessary files. It then cooks up some fancy URLs and sends them to the user\u0026#39;s device. end note SteeringService -\u0026gt; CDNs: Select Optimal CDNs activate CDNs CDNs --\u0026gt; SteeringService: Optimal CDNs deactivate CDNs SteeringService --\u0026gt; UserDevice: Send Fancy URLs deactivate SteeringService note left of UserDevice User\u0026#39;s device sends out requests to the chosen CDNs, starting the process of grabbing the files. end note UserDevice -\u0026gt; CDNs: Request Files activate CDNs CDNs --\u0026gt; UserDevice: Serve Requested Files deactivate CDNs note left of UserDevice CDNs step up and serve the files requested, making sure everything lands on the user\u0026#39;s device for a good time. end note @enduml Step 4: copy \u0026amp; paste *wsd output to PlantUML renderer I personally use VCCode with PlantUML plugin but you could use any PlantUML renderer you like. VSCode plugin: plantUML Visual result UML sequence Summary In this article I\u0026rsquo;ve outlined a simple and efficient approach to automate UML diagram creation using ChatGPT and PlantUML. The focus is on helping Solutions Architects and engineers quickly visualize some aspects of system designs without getting bogged down by the intricacies of PlantUML syntax\nThe tutorial walks you through a practical example: visualizing the HLD of Netflix. By following 4-steps, you can save time on learning PlantUML syntax and direct your efforts towards creating awesome system designs.\nHappy designing and saving even more time on routine tasks üïê\n","permalink":"https://juliakostrikova.com/posts/chatgpt/plantuml/","summary":"Introduction UML diagrams play a crucial role in software engineering for visualizing and structuring information. This article explores the seamless automation of UML diagram creation using ChatGPT (LLM) and the PlantUML tool. The aim is to provide a valuable resource for Solutions Architects and engineers looking to efficiently communicate and pitch ideas through visual representations.\nIn this tutorial, we\u0026rsquo;ll focus on visualization of the high-level system design of Netflix. By the end of this article, you\u0026rsquo;ll have a clear understanding of how ChatGPT and PlantUML can be harnessed to expedite the visualization of systems design while saving time on learning PlatUML syntax.","title":"Automating UML Diagrams with ChatGPT and PlantUML"},{"content":"EBS Volumes Storage volumes you can attach to your EC2 instances.\nMission Critical üü¢ production workloads. designed for mission critical workloads\nüü¢ highly available. automatically replicated within a single AZ to protect against hardware failures.\nüü¢ scalable. dynamically increase capacity and chnage the volume type with no downtime or performance impact to your live systems.\nEBS Volume Types: Solid State Disk General Purpose SSD (gp2) - legacy 3 IOPS per GiB, up to maximum of 16,000 IOPS per volume regardless of how large the volume is, the maximum number of input/output operations the storage system can handle for that volume is 16,000 per second. this is the upper limit on the performance you can expect from the volume.\ngp2 volumes smaller than 1TB can burst up to 3,000 IOPS\ngood for boot volumes or development and test applications that are not latency sensitive\nGeneral Purpose SSD (gp3) predictable 3,000 IOPS baseline performance and 125 MiB/s regardless of volume size\nideal for applications that require high performace but not high storage capacity, such as MySQL, Cassandra, and Hadoop clusters.\ncustomers looking for higher performance can scale up to 16,000 IOPS and 1,000 MiB/s for an additional fee.\nthe top performance of gp3 is 4 times faster then max throughput of gp2 volumes.\nProvisioned IOPS SSD (io1) - legacy high-performance option and the most expensive\nup to 64, 000 IOPS per volume. 50 IOPS per GiB\nuse if you need \u0026gt; 16,000 IOPS\ndesigned for I/O-intensive applications, large databases, and latency-sensitive workloads.\nProvisioned IOPS SSD (io2) applications that need high levels of durability\nlatest generation. 500 IOPS per GiB up to 64,000 IOPS\n99,999% durability instead of up to 99,9%\nI/O intensive apps, large databases, and latency-sensitive workloads.\nEBS Volume Types: Hard Disk Drive (MB/s-intensive) Throughput Optimized HDD (st1) low-cost HDD volume\nbaseline throughput of 40 MB/s per TB\nability to burst up to 250 MB/s per TB\nmaximum throughput of 500 MB/s per volume\nfrequently accessed, throughput-intensive workloads\nbig data, data warehouses, ETL, and log processing\na cost-affective way to store mountains of data\n‚ùï cannot be a boot volume\nCold HDD (sc1) baseline throughput of 12 MB/s per TB\nability to burst up to 80 MB/s per TB\nmaximum throughput of 250 MB/s per volume\ngood choice for colder data requiring fewer scans per day\nbig data, data warehouses, ETL, and log processing\ngood for applications that need the lower cost and performance is not a factor\n‚ùï cannot be a boot volume\nIOPS vs Throughput IOPS Throughput measures the number of read and write operations per second measures the number of bits read or written per second (MB/s) important metric for qwick transations, low-latency apps, transactional workloads important metricfor large datasets, large I/O sizes, complex queries the ability to action reads and writes very qwickly the ability to deal with large datasets choose Provisioned SSD (io1 or io2) choose Throughput Optimized HDD (st1) Volumes and Snapshots What are Volumes? exists on EBS\nStorage drives (or volumes as they‚Äôre described in AWS documentation) are for the most part virtualized spaces carved out of larger physical drives. To the OS running on your instance, however, all AWS volumes will present themselves exactly as though they were normal physical drives.\nWhat are Snapshots? exists on S3\nthink of snapshot as a photograph of the virtual disk/volume snapshots are point in time when you take a snapshot, it is a point-in-time copy of a volume.\nsnapshots are incremental this means only the data that has been changed since your last snapshot are moved to S3. this saves on space and the time to take a snapshot.\nEncrypting Volumes data at rest is encrypted inside the volume\nall data in flight moving between the instance and vthe volume is encryped\nall snapshots are encrypted.\nall volumes created from the snapshot are encrypted\n4 Steps to Encrypt an Unecrypted Volume Create a snapshot of unencrypted root device volume Create a copy of the snapshot and select the encrypt option. Create an AMI from the encrypted snapshot. Use the AMI to launch new encrypted instances. References Migrate your Amazon EBS volumes from gp2 to gp3 and save up to 20% on costs\n","permalink":"https://juliakostrikova.com/posts/saa-c03/ebs_efs/","summary":"EBS Volumes Storage volumes you can attach to your EC2 instances.\nMission Critical üü¢ production workloads. designed for mission critical workloads\nüü¢ highly available. automatically replicated within a single AZ to protect against hardware failures.\nüü¢ scalable. dynamically increase capacity and chnage the volume type with no downtime or performance impact to your live systems.\nEBS Volume Types: Solid State Disk General Purpose SSD (gp2) - legacy 3 IOPS per GiB, up to maximum of 16,000 IOPS per volume regardless of how large the volume is, the maximum number of input/output operations the storage system can handle for that volume is 16,000 per second.","title":"SAA-C03:: AWS Storage. EBS \u0026 EFS"},{"content":"S3 is Object-Based Storage Manages data as objects rather then in file systmes or data blocks.\nUpload any file type you can think of to S3. Examples include photos, videos, code, documents, and text files. Cannot be used to run an OS or DB. S3 Basics Unlimited storage. The total amount of data and the number of objects you can store is unlimited. Objects up to 5 TB in Size. S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. S3 Buckets. Store files in buckets (similar to folders). Tired Storage. Offers a range of storage classes designed for different use cases. Lifecycle Management. Defines automatically transition objects to a cheaper tier or delete objects that are no longer required after a set of period of time. Versioning. All versions of an object are stored and can be retrived, including deleted objects. Once enabled, versioning cannot be disabled - only suspended. Supports MFA, so you need to procced with 2-factor in order to delete an object. If you enabled public access to versioned objects, old versions will not be accessible. Working with S3 Buckets Universal Namespace. All AWS accounts share the S3 namespace. Each S3 bucket name is globally unique.\nExample S3 URLs\nhttps://bucket-name.s3.Region.amazonaws.com/key-name Key-Value Store Key the name of object (e.g., Toba.jpg) Value the data itself, which is made up of a sequence of bytes Version ID important for storing multiple versions of the same object Metadata data about the data you are storing (e.g., content-type, last-modified, etc.)\nHight Available and Highly Durable build for 99.95 - 99.99% service availability, depending on the S3 tier. designed for 99.999999999 (9 decimal places) durability for data stored in S3. data stored redundantly across multiple devices in multiple facilities Securing your data üîí Buckets are private by default. You have to apply public access on both the bucket and its objects in order to make the bucket public.\nEncryption üîë 1.1 Encryption in Transit\nSSl/TLS HTTPS 1.2 Encryption at Rest: Server-Side Encryption\nSSE-S3 : S3-managed keys, using AES 256-bit encryption SSE-KMS : AWS Key Management Service-managed keys SSE-C : Customer-provided keys 1.3 Encryption at Rest: Client-Side Encryption\nencrypt you files yourself before uploading to S3 Access Control List (ACLs) Define which AWS accounts or groups are granted access and the type of access. You can attach S3 ACLs to individial objects within a bucket. Bucket Policies. S3 bucket policies specify what actions are allowed or denied (e.g., allow user Alice to PUT but not DELETE objects in the bucket.) Bucket policies work on an entire bucket level. Enforcing Server-Side Encryption 1Ô∏è‚É£ Console. Select encryption settings in S3 Bucket.\n2Ô∏è‚É£ Bucket Policy.\nx-amz-server-side-encryption: AES256 - (SSE-S3 - S3 managed keys)\nx-amz-server-side-encryption: aws:kms - (SSE-KMS - KMS managed keys)\nüí° You can create abucket policy that denies any S3 PUT request that doesn\u0026rsquo;t include the x-amz-server-side-encryption parameter in request header.\nPUT request\nPUT /my-image.jpg HTTP/1.1 Host: myBucket.s3.\u0026lt;Region\u0026gt;.amazonaws.com Date: Wed, 12 Oct 2009 17:50:00 GMT Authorization: authorization string Content-Type: text/plain Content-Length: 11434 x-amz-meta-author: Janet Expect: 100-continue x-amz-server-side-encryption: AES256 [11434 bytes of object data] Static Websites on S3 S3 scales automatically to meet demand Many enterprises will put static websites on S3 when they think there is going to be a large number or requests.\nS3 Storage Classes S3 Standard data is stored redunduntly across multiple devices in multiple facilities (\u0026gt;= 3 AZs) designed for frequent access suitable for most workloads: the default storage class; use cases include websites, content distribution, mobile and gaming applications, and big data analatics. 99.99% availability 99.999999999 (11 9\u0026rsquo;s) durability S3 Standard-Infrequent Access (S3 Standard-IA) Use Case: great for long-term storage, backups and as data store for disaster recovery files\nrapid access used for data that is accessed less frequently but requires rapid access when needed. you pay to access data there is low per-GB storage price and a per-GB retrieved fee. 99.9% availability 99.999999999 (11 9\u0026rsquo;s) durability S3 One Zone Infrequent Access Use Case: good for long-lived, infrequently accessed, non-critical data\nLike S3 Standard-IA, but data is stored redunduntly within single AZ.\ncost 20% less then regular S3 Standard-IA 99.5% availability 99.999999999 (11 9\u0026rsquo;s) durability S3 Intelligent-Tiering Use Case: If you don;t know whether you\u0026rsquo;ll be accessing data frequently or infrequently\nüíµ optimization: montly fee of $0.0025 per 1,000 objects\nautomatically moves your data to the most cost-effective tier based on how frequently you access each object. 99.99% availability 99.999999999 (11 9\u0026rsquo;s) durability 3 Glacier Options you pay each time you access data use only for archieving data glacier is cheap storage optimized for data that is very infrequently accessed 99.99% availability 99.999999999 (11 9\u0026rsquo;s) durability Option 1: Glacier Instant Retrieval provides long-term data archiving with instant retrieval time for your data.\nOption 2: Glacier Flexible Retrieval ideal storage class for archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases. can be minites or up to 12 hours.\nOption 3: Glasier Deep Archive cheapest storage class and designed for customers that retain data sests for 7-10 years or longer to meet customer needs and regulatory compliance requirements. the standard retrieval time is 12 hours and the bulk retrieval time is 48 hours.\nPerformance across the S3 Storage Classes S3 Standard S3 Intelligent-Tiering S3 Standard-IA S3 One Zone-IA+ S3 Glacier Instant Retrieval S3 Glacier Flexible Retrieval S3 Glacier Deep Archive Designed for Durability 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) 99.999999999% (11 9\u0026rsquo;s) Designed for Availability 99.99% 99.90% 99.90% 99.50% 99.99% 99.99% 99.99% Availability SLA 99.90% 99% 99% 99% 99.90% 99.90% 99.90% AZs \u0026gt;=3 \u0026gt;=3 \u0026gt;=3 1 \u0026gt;=3 \u0026gt;=3 \u0026gt;=3 Min Capacity charge per Object n/a n/a 128KB 128KB 128KB 40KB 40KB Min Storage duration charge n/a 30 days 30 days 30 days 90 days 90 days 180 days Retrieval Fee n/a n/a Per GB retrieved Per GB retrieved Per GB retrieved Per GB retrieved Per GB retrieved Storage Type Object Object Object Object Object Object Object Lifecycle Transitions Yes Yes Yes Yes Yes Yes Yes Storage Classes - Costs üíµ S3 Standard general-purpose for any type of data, typically used for frequent access\nFirst 50 TB / Month 0.023$ per GB Next 450 TB / Month 0.022$ per GB Over 500 TB / Month 0.021$ per GB S3 Intelligent-Tiering cost saving strategies applied for data with unknown or changing access patterns\nno min storage duration\nFirst 50 TB / Month 0.023$ per GB Next 450 TB / Month 0.022$ per GB Over 500 TB / Month 0.021$ per GB All Storage / Month 0.0025$ per GB Monitoring and Automation, All Storage / Month 0.0025$ per 1000 Objects S3 Standard Infrequent Access long-lived but infrequently accessed data (once a month) that needs milliseconds access 30 days min storage duration\nAll Storage / Month 0.0125$ per GB S3 One Zone-Infrequent Access re-creatable infrequently accessed data that needs milliseconds access 30 days min storage duration\nAll Storage / Month 0.01$ per GB S3 Glacier long-lived archived data accssed once a quarter with instant retrieval in milliseconds\nAll Storage / Month 0.004$ per GB S3 Glacier Flexible Retrieval archived data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost. retrieval time: can be minutes or up to 12 hours 90 days min storage duration\nAll Storage / Month 0.0036$ per GB S3 Glacier Deep Archive designed to retain data sets for 7-10 years or longer. retrieval time: 12 hours and the bulk is 48 hours 180 days min storage duration\nAll Storage / Month 0.00099$ per GB Lifecycle Management automate moving object between the different storage tiers, thereby maximazing cost effectiveness. can be used in conjuction with versioning can be applied to current and noncurrent versions Example: S3 Standard: keep for 30 days -\u0026gt; S3 IA: after 30 days -\u0026gt; Glacier: After 90 days\nS3 Object Lock üîí you can use S3 Object Lock to store objects using a write once, read many model. It can help to prevent objects from being delete or modified for a fixed amount of time or indefinitely. object lock can be on individual objects or applied across the bucket as a whole object lock comes in two modes: Governance Mode, Complience Mode S3 Object Lock Modes Governance Mode\nUsers can\u0026rsquo;t overwrite or delete an object version or alter its lock settings unless they have special permissions.\nComplience Mode\nA protected object version can\u0026rsquo;t be overwritten or deleted by any user, including the root in your AWS Account. Retention mode of object can\u0026rsquo;t be changed and its retenton period can\u0026rsquo;t be shortened. Compiance mode ensures an object version can\u0026rsquo;t be overriten or deleted for the duration of the retention period.\nRetention Periods üï• Retention period protects an object version for a fixed amount of time. Ehen ypu place a retention period on an object version, Amazon S3 stores a timestamp in the object version\u0026rsquo;s metadata to indicate when the retention period expires.\nüí° After retention period expires, the object version can be overriten or deleted unless you also placed a legal hold on object version.\nLegal Holds Like retention period, a legal hold prevents an object version from being overriten or deleted.\nHowever, a legal holds doesn\u0026rsquo;t have an assosiated retention period and remains in effect untill removed.\nA Legal Hold can be placed and removed by any user who has s3:PutObjectLegalHold permission.\nGlacier Vault Lock Allows to deploy and enforce compiance controls for individual S3 Glacier vaults with a vault lock policy.\nyou can specify controls such as WORM, in a vault lock policy.\nüî¥ once locked, the policy can\u0026rsquo;t be changed\nOptimizing S3 Performance S3 Prefixes this is folders inside you bucket. it doesn\u0026rsquo;t include object name\nbucketname/folder1/subfolder1/file.md -\u0026gt; prefix: /folder1/subfolder1/ bucketname/folder2/subfolder1/file.md -\u0026gt; prefix: /folder2/subfolder1/ bucketname/folder3/file.md -\u0026gt; prefix: /folder3 S3 Performance S3 has low latency. You can get the first byte out of S3 within 100-200 milliseconds.\nYou can achive a hight number of requests:\n3,500 PUT/COPY/POST/DELETE 5,500 GET/HEAD requests per second, pre prefix.\nüí° the more prefixes you have on the bucket, the higher performance you can get. for example, 2 prefixes gives you 11,000 rps; 4 prefixes = 22,000; etc.\nLimitations with KMS üü† SSE-KMS; with file upload, you will call GenerateDataKey in the KMS API. -\u0026gt; KMS quota (can\u0026rsquo;t be increased)\nüü† SSE-KMS; with file download you will call Decrypt in the KMS API. -\u0026gt; KMS quota (can\u0026rsquo;t be increased)\nbetter to use native S3 encryption (SSE-S3) rather then KMS\nUploads üü¢ Multipart Uploads\nrecommended for files over 100 MB required for files over 5 GB parallelize uploads (increase efficiency) Downloads üü¢ S3 Byte-Range Fetches\nparallelize downloads by specifying byte ranges. if there is a failure in the download, it it\u0026rsquo;s only for a specific byte range. Backup Data with S3 Replication you can replicate objects from one bucket to another.versioning must be enabled for both sides objects in an existing bucket are not replicated automatically delete markers are not replicated by default ","permalink":"https://juliakostrikova.com/posts/saa-c03/s3/","summary":"S3 is Object-Based Storage Manages data as objects rather then in file systmes or data blocks.\nUpload any file type you can think of to S3. Examples include photos, videos, code, documents, and text files. Cannot be used to run an OS or DB. S3 Basics Unlimited storage. The total amount of data and the number of objects you can store is unlimited. Objects up to 5 TB in Size. S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB.","title":"SAA-C03:: Simple Storage Service (S3)"},{"content":"Virtualization The technology that lies at the core of all cloud operations is virtualization. Virtualization lets you divide the hardware resources of a single physical server into smaller units. That physical server could therefore host multiple virtual machines (VMs) running their own complete operating systems, each with its own memory, storage, and network access.\na virtual machine host Cloud Computing Major cloud providers like AWS have enormous server farms where hundreds of thousands of servers and disk drives are maintained along with the network cabling necessary to connect them. A well-built virtualized environment could provide a virtual server using storage, memory, compute cycles, and network bandwidth collected from the most efficient mix of available sources it can find. A cloud computing platform offers on-demand, self-service access to pooled compute resources where your usage is metered and billed according to the volume you consume. Cloud computing systems allow for precise billing models, sometimes involving fractions of a penny for an hour of consumption.\nAWS Global Infrastructure At the time of writing this note, The AWS Cloud spans ‚¨áÔ∏è\n102 Availability Zones within 32 geographic regions around the world 400+ Edge Locations and 13 Regional Edge Caches üí° with announced plans for 15 more Availability Zones and 5 more AWS Regions in Canada, Israel, Malaysia, New Zealand, and Thailand.\nAvailability Zones / Data Centers An Availability Zone (AZ) is a data center. AZ is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region.\nAll AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. All traffic between AZs is encrypted.\nIf an application is partitioned across AZs, companies are better isolated and protected from issues such as\npower outages, lightning strikes, tornadoes, earthquakes, and more. AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles) of each other.\nRegions A region is a geographical area. Each Region consists of 2 (or more) Availability Zones.\nEdge Networking Edge networking in AWS refers to the concept of distributing content, applications, and services closer to end-users or devices. This approach aims to reduce latency and improve performance by minimizing the distance data needs to travel. AWS offers several services and features that support edge networking, such as\nAmazon CloudFront, AWS Global Accelerator, Amazon Route 53. These services reside at AWS‚Äô global edge locations connected by dedicated 100Gbps redundant fiber .\nAmazon CloudFront This is a content delivery network (CDN) service that caches and delivers content, including\nweb pages, images, videos, other assets from multiple edge locations around the world. It helps ensure that users can access content with minimal delay, regardless of their geographical location.\nAWS Global Accelerator This service allows you to direct traffic over the AWS global network infrastructure, dynamically routing requests to the nearest and most optimal AWS endpoint. It improves the availability and performance of applications by leveraging Anycast routing.\nIn Anycast routing, multiple servers or network nodes are configured with the same IP address. When a user sends a request to that IP address, the network routes the request to the nearest available node using routing protocols and metrics such as network topology, latency, and other factors.\nAmazon Route 53 AWS\u0026rsquo;s domain name system (DNS) service, Route 53, can be used to route traffic based on geographic location, latency, health, or other routing policies. This helps optimize the distribution of traffic and ensures efficient access to resources.\nIn fact, it focuses on four distinct areas:\nDomain registration DNS management Availability monitoring (health checks) Traffic management (routing policies) Route 53 now also provides an Application Recovery Controller through which you can configure recovery groups, readiness checks, and routing control.\nSecurity and Identity Identity and Access Management (IAM) You use IAM to administer user and programmatic access and authentication to your AWS account. Through the use of users, groups, roles, and policies, you can control exactly who and what can access and/or work with any of your AWS resources.\nKey Management Service (KMS) KMS is a managed service that allows you to administrate the creation and use of encryption keys to secure data used by and for any of your AWS resources.\nDirectory Service For AWS environments that need to manage identities and relationships, Directory Service can integrate AWS resources with identity providers like Amazon Cognito and Microsoft AD domains.\nCompute EC2 The central focus within a conventional datacenter or server room revolves around its invaluable servers. However, to harness the utility of these servers, one must incorporate elements such as racks, power supplies, cabling, switches, firewalls, and cooling systems. Amazon Web Services\u0026rsquo; (AWS) Elastic Compute Cloud (EC2) is meticulously crafted to faithfully emulate the experience of a datacenter or server room. At its core lies the EC2 virtual server, referred to as an \u0026ldquo;instance.\u0026rdquo; Yet, akin to the scenario of a local server room described earlier, AWS offers an array of services meticulously designed to bolster and refine the operations of your workloads. These encompass tools for overseeing and managing resources, alongside purpose-built platforms catering to the coordination of containers.\nLambda At its core, AWS Lambda allows developers to encapsulate discrete pieces of functionality into self-contained units known as functions. These functions can be triggered by a multitude of events, ranging from HTTP requests and changes to data stored in Amazon S3, to updates in databases and modifications in the AWS Management Console. Each function executes in a completely isolated environment, with its own compute resources and dependencies, ensuring optimal performance and scalability.\nElastic Beanstalk Beanstalk is a managed service that abstracts the provi- sioning of AWS compute and networking infrastructure. You are required to do nothing more than push your application code, and Beanstalk automatically launches and manages all the necessary services in the background.\nAuto Scaling Copies of running EC2 instances can be defined as image templates and automatically launched (or scaled up) when client demand can‚Äôt be met by existing instances. As demand drops, unused instances can be terminated (or scaled down).\nElastic Load Balancing Incoming network traffic can be directed between multiple web servers to ensure that a single web server isn‚Äôt overwhelmed while other servers are underused or that traffic isn‚Äôt directed to failed servers.\nElastic Container Service Compute workloads using container technologies like Docker and Kubernetes can be provisioned, automated, deployed, and administered using full integration with your other AWS account resources. Kubernetes workloads have their own environment: Amazon Elastic Kubernetes Service (EKS).\nStorage S3 S3 offers highly versatile, reliable, and inexpensive object storage that‚Äôs great for data storage and backups. It‚Äôs also commonly used as part of larger AWS production processes, including through the storage of script, template, and log files.\nS3 Glacier A good choice for when you need large data archives stored cheaply over the long term and can live with retrieval delays measuring in the hours. Glacier‚Äôs life cycle management is closely integrated with S3.\nEBS EBS provides the persistent virtual storage drives that host the operating systems and working data of an EC2 instance. They‚Äôre meant to mimic the function of the storage drives and partitions attached to physical servers.\nEBS-Provisioned IOPS SSD If your applications will require intense rates of I/O operations, then you should consider provisioned IOPS. There are currently three flavors of provisioned IOPS volumes: io1, io2, and io2 Block Express. io1 can deliver up to 50 IOPS/GB to a limit of 64,000 IOPS (when attached to an AWS Nitro‚Äìcompliant EC2 instance) with a maximum throughput of 1,000 MB/s per volume. io2 can provide up to 500 IOPS/GB. And io2 Block Express can give you 4,000 MB/s throughput and 256,000 IOPS/volume.\nEBS General-Purpose SSD For most regular server workloads that, ideally, deliver low-latency performance, general- purpose SSDs will work well. You‚Äôll get a maximum of 3,000 IOPS/volume. For refer- ence, assuming 3,000 IOPS per volume and a single daily snapshot, a general-purpose SSD used as a typical 8 GB boot drive for a Linux instance would, at current rates, cost you $3.29/month.\nHDD Volumes For large data stores where quick access isn‚Äôt important, you can find cost savings using older spinning hard drive technologies. The sc1 volume type provides the lowest price of any EBS storage ($0.015/GB-month). Throughput optimized hard drive volumes (st1) are available for larger stores where infrequent bursts of access at a rate of 250 MB/s per TB are sufficient. st1 volumes cost $0.045/GB per month. The EBS Create Volume dialog box cur- rently offers a Magnetic (Standard) volume type.\nEFS The Elastic File System (EFS) provides automatically scalable and shareable file storage to be accessed from Linux instances. EFS-based files are designed to be accessed from within a virtual private cloud (VPC) via Network File System (NFS) mounts on EC2 Linux instances or from your on-premises servers through AWS Direct Connect connections. The goal is to make it easy to enable secure, low-latency, and durable file sharing among multiple instances.\nFSx Amazon FSx comes in four flavors: FSx for Lustre, FSx for Windows File Server, FSx for OpenZFS, and FSx for NetApp ONTAP. Lustre is an open source distributed filesystem built to give Linux clusters access to high-performance filesystems for use in compute-intensive operations. As with Lustre, Amazon‚Äôs FSx service brings OpenZFS and NetApp filesystem capabilities to your AWS infrastructure. FSx for Windows File Server, as you can tell from the name, offers the kind of file-sharing service EFS provides but for Windows servers rather than Linux. FSx for Windows File Server integrates operations with Server Message Block (SMB), NTFS, and Microsoft Active Directory.\nStorage Gateway Storage Gateway is a hybrid storage system that exposes AWS cloud storage as a local, on-premises appliance. Storage Gateway can be a great tool for migration and data backup and as part of disaster recovery operations.\nAWS Snow Family Migrating large data sets to the cloud over a normal Internet connection can sometimes require far too much time and bandwidth to be practical. If you‚Äôre looking to move terabyte- or even petabyte-scaled data for backup or active use within AWS, ordering a Snow device might be the best option. When requested, AWS will ship you a physical, appropriately sized device to which you transfer your data. When you‚Äôre ready, you can then send the device back to Amazon. Amazon will then transfer your data to buckets on S3.\nAWS DataSync DataSync specializes in moving on-premises data stores into your AWS account with a minimum of fuss. It works over your regular Internet connection, so it‚Äôs not as useful as Snowball for really large data sets. But it is much more flexible, since you‚Äôre not limited to S3 (or RDS as you are with AWS Database Migration Service). Using DataSync, you can drop your data into any service within your AWS account. That means you can do the following: Quickly and securely move old data out of your expensive datacenter into cheaper S3 or Glacier storage. Transfer data sets directly into S3, EFS, or FSx, where it can be processed and analyzed by your EC2 instances. Apply the power of any AWS service to any class of data as part of an easy-to-configure automated system. A single DataSync task can handle external transfer rates of up to 10 Gbps (assuming your connection has that capacity) and offers both encryption and data validation.\nDatabases RDS Amazon Relational Database Service (RDS) is a managed database service that lets you run relational database systems in the cloud. RDS takes care of setting up the database system, performing backups, ensuring high availability, and patching the database software and the underlying operating system. RDS also makes it easy to recover from database failures, restore data, and scale your databases to achieve the level of performance and availability that your application requires.\nDynamoDB DynamoDB is a managed nonrelational database service that can handle thousands of reads and writes per second. It achieves this level of performance by spreading your data across multiple partitions. A partition is a 10 GB allocation of storage for a table, and it‚Äôs backed by solid-state drives in multiple availability zones.\nRedshift Redshift is Amazon‚Äôs managed data ware- house service. Although it‚Äôs based on PostgreSQL, it‚Äôs not part of RDS. Redshift uses columnar storage, meaning that it stores the values for a column close together. This improves storage speed and efficiency and makes it faster to query data from individual col- umns.\nNetworking VPCs Like a traditional network, a VPC consists of at least one range of contiguous IP addresses. This address range is represented as a Classless Inter-Domain Routing (CIDR) block. The CIDR block determines which IP addresses may be assigned to instances and other resources within the VPC. You must assign a primary CIDR block when creating a VPC. After creating a VPC, you divide the primary VPC CIDR block into subnets that hold your AWS resources.\nDirect Connect The AWS Direct Connect service uses PrivateLink to offer private, low-latency connec- tivity to your AWS resources. One advantage of Direct Connect is that it lets you bypass the Internet altogether when accessing AWS resources, letting you avoid the unpredictable and high latency of a broadband Internet connection. This approach is useful when you need to transfer large data sets or real-time data, or you need to meet regulatory requirements that preclude transferring data over the Internet.\nApplication Integration API Gateway If you need to build an application that consumes data or other resources existing within the AWS platform, you can make reliable and safe connections using either RESTful or WebSocket APIs. API Gateway lets you publish data coming out of just about any AWS ser- vice for use in your IoT device fleets, web or mobile applications, or monitoring dashboards.\nSimple Notification Service (SNS) SNS is a notification tool that can automate the publishing of alert topics to other services (to an SQS Queue or to trigger a Lambda function, for instance), to mobile devices, or to recipients using email or SMS.\nSimple Workflow (SWF) SWF lets you coordinate a series of tasks that must be per-formed using a range of AWS services or even nondigital (meaning human) events.\nSimple Queue Service (SQS) SQS allows for event-driven messaging within distributed systems that can decouple while coordinating the discrete steps of a larger process.The data contained in your SQS messages will be reliably delivered, adding to the fault- tolerant qualities of an application.\nApplication management CloudWatch CloudWatch can be set to monitor process performance and resource utilization and, when preset thresholds are met, either send you a message or trigger an automated response.\nCloudFormation This service enables you to use template files to define full and complex AWS deployments. The ability to script your use of any AWS resources makes it easier to automate, stan- dardizing and speeding up the application launch process.\nCloudTrail CloudTrail collects records of all your account‚Äôs API events. This history is useful for account auditing and troubleshooting purposes.\nConfig The Config service is designed to help you with change management and compliance for your AWS account. You first define a desired configuration state, and Config evalu- ates any future states against that ideal. When a configura- tion change pushes too far from the ideal baseline, you‚Äôll be notified.\nAWS Well-Architected Framework 1. Operational excellence Perform operations as code Make frequent, small, reversible changes Refine operations procedures frequently Anticipate failure Learn from all operational failures 2. Security Implement a strong identity foundation (least privilege principle) Maintain traceability Apply security at all layers Automate security best practices Implementation of controls that are defined and managed as code in version-controlled templates\nProtect data in transit and at rest Keep people away from data Prepare for security events Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery\n3. Reliability Automatically recover from failure Test recovery procedures Scale horizontally to increase aggregate workload availability Replace one large resource with multiple small resources to reduce the impact of a single failure on the overall workload. Distribute requests across multiple, smaller resources to verify that they don‚Äôt share a common point of failure.\nStop guessing capacity In on-premises scenarios, resource saturation leading to workload failure is often caused by exceeding capacity, while in the cloud, monitoring and automated adjustments of resources enable efficient demand satisfaction without over- or under-provisioning, within certain limits and managed quotas.\nManage change in automation 4. Performance efficiency Democratize advanced technologies Make advanced technology implementation smoother for your team by delegating complex tasks to your cloud vendor\nGo global in minutes Use serverless architectures Experiment more often Consider mechanical sympathy Understand how cloud services are consumed and always use the technology approach that aligns with your workload goals. For example, consider data access patterns when you select database or storage approaches.\n5. Cost optimization Implement Cloud Financial Management Adopt a consumption model Measure overall efficiency Stop spending money on undifferentiated heavy lifting Analyze and attribute expenditure You can use cost allocation tags to categorize and track your AWS usage and costs. When you apply tags to your AWS resources (such as EC2 instances or S3 buckets), AWS generates a cost and usage report with your usage and your tags.\n6. Sustainability Understand your impact Establish sustainability goals Maximize utilization Anticipate and adopt new, more efficient hardware and software offerings Use managed services Reduce the downstream impact of your cloud workload References\nhttps://www.wellarchitectedlabs.com\n","permalink":"https://juliakostrikova.com/posts/saa-c03/aws-fundamentals/","summary":"Virtualization The technology that lies at the core of all cloud operations is virtualization. Virtualization lets you divide the hardware resources of a single physical server into smaller units. That physical server could therefore host multiple virtual machines (VMs) running their own complete operating systems, each with its own memory, storage, and network access.\na virtual machine host Cloud Computing Major cloud providers like AWS have enormous server farms where hundreds of thousands of servers and disk drives are maintained along with the network cabling necessary to connect them.","title":"SAA-C03:: Introduction to Cloud Computing and AWS"},{"content":"My Use case To establish a seamless synchronization of project files between a MacBook (working station) and a Raspberry Pi (hosting server), allowing for automatic updates and accessibility of the project from both devices.\ndevice/tool version DS420+ DSM 7.2-64570 Raspberry PI 400 The user begins working on the project using the IDE on the MacBook, editing and updating project files within the synchronized folder. As changes are made, they are automatically reflected in the synchronized folder and accessible on the Raspberry Pi in read-only mode.\nnasnfs 1. Create folder on NAS 1.1 Set up NFS rules To start with, create a shared folder Control Panel -\u0026gt; Create -\u0026gt; Create Shared Folder Follow all steps though Shared Folder Creation Wizard panel. Once you are done, select your folder (in my case it is yuklia_projects) click on Edit, navigate to NFS permissions tab. Create NFS rule.\nrule value note Hostname or IP raspberrypi_local_ip üîí limit access only to trusted IP to reduce the surface attack. Priviledge Read only read-only since I don‚Äôt need my files to be changed on the raspberryPi side. Squash Map all users to guest Assigns access privileges to all users of NFS client equivalent to the guest access privileges on your system. Security AUTH_SYS Use the NFS client\u0026rsquo;s UID (user identifier) and GID (group identifier) to check access permissions. 2. SSH to raspberryPI 2.1 Connect to your PI ssh username@raspberrypi_local_ip 2.2 Create mount point sudo mkdir /mnt/nas_yuklia_projects 2.3 Mount folder sudo mount -t nfs {SYNOLOGY_IP}:{MOUNT_POINT} /mnt/nas_yuklia_projects MOUNT_POINT - open Control Panel -\u0026gt; Shared Folder -\u0026gt; Edit SharedFolder, navigate to NFS permissions tab, in the left bottom side you\u0026rsquo;ll see Mount path: /volume1/yuklia_projects (in my case)\nSYNOLOGY_IP - local IP address of you Synology device\n3. Validation ‚úÖ Execute df -h to display the list of mounted points; your mounted point should be visible.\n‚úÖ Attempt to modify any files on the Raspberry Pi; you should encounter the warning:\nError writing lock file /mnt/nas_yuklia_projects/.test3.md.swp: Read-only file system 4. Security Measures for NFS üõ°Ô∏è NFS (Network File System) is not inherently secure when used in its basic configuration. It was designed with a focus on ease of use and performance rather than security. However, there are ways to enhance the security of NFS:\n1Ô∏è‚É£ Use NFS Versions with Security Features:\nNFSv4 introduced significant security enhancements compared to older versions (NFSv2 and NFSv3). It includes features like strong authentication and encryption. It\u0026rsquo;s highly recommended to use NFSv4 or later versions for better security.\n2Ô∏è‚É£ Kerberos Authentication:\nImplement Kerberos-based authentication for NFSv4. Kerberos provides secure authentication, preventing unauthorized access to the NFS shares.\n3Ô∏è‚É£ Firewall Configuration:\nUse firewalls to control and restrict access to NFS services. Limit access only to trusted IP addresses or subnets to reduce the attack surface. Network Segmentation:\n4Ô∏è‚É£ Network Segmentation:\nPlace NFS servers on a separate network segment, isolating them from potentially insecure parts of the network. This can help contain security breaches.\n5Ô∏è‚É£ Access Controls:\nUtilize NFS access controls effectively. Set appropriate permissions and access rules on the NFS server to limit access to authorized users and hosts.\n6Ô∏è‚É£ Secure the Underlying Operating System: Ensure that the operating system on both the NFS server and client is securely configured. Regularly apply security patches and updates to address any known vulnerabilities.\n7Ô∏è‚É£ VPN or SSH Tunneling: Use a VPN (Virtual Private Network) or SSH (Secure Shell) tunneling when accessing NFS over untrusted networks. This adds an additional layer of encryption and security.\nSummary In this guide, I show you how to seamlessly sync project files between a MacBook and a Raspberry Pi using Synology NAS and NFS. The step-by-step process ensures updates from the MacBook are automatically reflected on the Raspberry Pi, granting read-only access. Happy syncing!\n","permalink":"https://juliakostrikova.com/posts/homelab/ovderride-provider/","summary":"My Use case To establish a seamless synchronization of project files between a MacBook (working station) and a Raspberry Pi (hosting server), allowing for automatic updates and accessibility of the project from both devices.\ndevice/tool version DS420+ DSM 7.2-64570 Raspberry PI 400 The user begins working on the project using the IDE on the MacBook, editing and updating project files within the synchronized folder. As changes are made, they are automatically reflected in the synchronized folder and accessible on the Raspberry Pi in read-only mode.","title":"How to Mount Synology NAS to RaspberryPi via NFS"},{"content":"Traefik v1.7 is running within a Docker Swarm, functioning as a global service. Consequently, an instance of Traefik is deployed for each host within our Docker Swarm cluster. It is important to note that the Swarm\u0026rsquo;s routing mesh is not employed in this setup. Instead, the port mode is configured as \u0026ldquo;host,\u0026rdquo; allowing the port to be attached to the host\u0026rsquo;s network.\ntraefik: image: ${REGISTRY}/traefik:1.6 build: ./traefik volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - traefik environment: - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} - AWS_REGION=${AWS_REGION} - AWS_HOSTED_ZONE_ID=${AWS_HOSTED_ZONE_ID} ports: - target: 80 published: 80 protocol: tcp mode: host - target: 443 published: 443 mode: host protocol: tcp - target: 8080 published: 8080 mode: host protocol: tcp deploy: mode: global restart_policy: condition: on-failure Docker Swarm deployed to AWS and lives behind internal/external load balancers (Classic type).\n// terraform SG fragment ingress { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;acme chellange\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } ingress { from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; description = \u0026#34;SSL termination\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } Let\u0026rsquo;s assume mydomain.com/mobile-api should be worldwide visible but the rest of app should be available via Whitelist. Hopefully Traefik supports entrypoints by route, so there is nothing easiest then make one more clone of app and set frontend rule targeting to /route.\nmodile-api: image: ${REGISTRY}/app:${VERSION:-latest} networks: - traefik deploy: mode: replicated replicas: 1 restart_policy: condition: on-failure labels: - traefik.backend=modile-api - traefik.port=80 - traefik.frontend.rule=Host:${HOST_API};PathPrefix:/mobile-api - traefik.docker.network=traefik - traefik.enable=true app: image: ${REGISTRY}/app:${VERSION:-latest} networks: - traefik deploy: mode: replicated replicas: 1 restart_policy: condition: on-failure labels: - traefik.backend=app - traefik.port=80 - traefik.frontend.whiteList.sourceRange=${WHITE_LIST} - traefik.frontend.whiteList.useXForwardedFor=true - traefik.frontend.rule=Host:${HOST_API} - traefik.docker.network=traefik - traefik.enable=true Important part: set up ProxyProtocol on both side Traefik (reverse-proxy) and AWS ELB. Proxy protocol Traefik side: traefik.toml\n[entryPoints.https.whiteList] sourceRange = [\u0026#34;1.2.3.4/24\u0026#34;] # you can override this part on service level useXForwardedFor = true [entryPoints.https.proxyProtocol] trustedIPs = [\u0026#34;10.0.100.0/24\u0026#34;] # private subnet CIRD block or IPs [entryPoints.https.forwardedHeaders] trustedIPs = [\u0026#34;10.0.100.0/24\u0026#34;] # private subnet CIRD block or IPs Proxy protocol AWS side: (if your listener settings support Proxy Protocol! )\n1 # make sure proxyPolicy is disabled. =\u0026gt; \u0026#34;BackendServerDescriptions\u0026#34; : [] aws elb describe-load-balancer-policy-types 2 aws elb create-load-balancer-policy --load-balancer-name %lb_name% --policy-name %name% --policy-type-name ProxyProtocolPolicyType --policy-attributes AttributeName=ProxyProtocol,AttributeValue=true 3 aws elb set-load-balancer-policies-for-backend-server --load-balancer-name %lb_name% --instance-port 443 --policy-names %policy_name% 4 check (optional) aws elb describe-load-balancers --load-balancer-name %lb_name% debug: aws elb delete-load-balancer-policy --load-balancer-name %lb_name% --policy-name %policy_name% # get ELB IPs aws ec2 describe-network-interfaces --filters Name=description,Values=\u0026#34;ELB elb_name\u0026#34; --query \u0026#39;NetworkInterfaces[*].PrivateIpAddresses[*]\u0026#39; If everything goes as expected you‚Äôll see your IP in X-Forwarded-For header üéâ\nHostname: 6b27211a260c IP: 127.0.0.1 IP: 10.255.1.59 IP: 172.18.0.13 IP: 10.0.2.65 GET / HTTP/1.1 Host: domain.com User-Agent: redacted Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3 Accept-Encoding: gzip, deflate Accept-Language: en-US,en;q=0.9,ru-RU;q=0.8,ru;q=0.7,uk-UA;q=0.6,uk;q=0.5 Cache-Control: max-age=0 Connection: keep-alive Upgrade-Insecure-Requests: 1 X-Forwarded-For: you_real_ip X-Forwarded-Port: 80 X-Forwarded-Proto: https ","permalink":"https://juliakostrikova.com/posts/cloud/traefik-x-forwarded-for/","summary":"Traefik v1.7 is running within a Docker Swarm, functioning as a global service. Consequently, an instance of Traefik is deployed for each host within our Docker Swarm cluster. It is important to note that the Swarm\u0026rsquo;s routing mesh is not employed in this setup. Instead, the port mode is configured as \u0026ldquo;host,\u0026rdquo; allowing the port to be attached to the host\u0026rsquo;s network.\ntraefik: image: ${REGISTRY}/traefik:1.6 build: ./traefik volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - traefik environment: - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} - AWS_REGION=${AWS_REGION} - AWS_HOSTED_ZONE_ID=${AWS_HOSTED_ZONE_ID} ports: - target: 80 published: 80 protocol: tcp mode: host - target: 443 published: 443 mode: host protocol: tcp - target: 8080 published: 8080 mode: host protocol: tcp deploy: mode: global restart_policy: condition: on-failure Docker Swarm deployed to AWS and lives behind internal/external load balancers (Classic type).","title":"Traefik behind AWS ELB. X-Forwarded-For header"},{"content":"What you will learn today ü¶¶: what is Traefik and why I decisively recommend to use it with Docker Swarm how to show custom error page if Traefik returns 404 (service not found) How I met Traefik üòÅ Traefik is a top modern reverse proxy and load balancer. I started to use it from major version 1. At that moment, I already had Docker Swarm cluster, which I began to prepare for production use, and only one thing in the coherent picture has been left: \u0026ldquo;External Load Balancer\u0026rdquo;. Official docker doc had an example with HaProxy so it was obvious to try it out first. As I remember it took a significant amount of time to get HaProxy works smoothly with Swarm. But I wasn\u0026rsquo;t satisfied with the result:\nstatic config which is needed to re-deploy in case of changes *.cfg not supports variables substitution (out of the box) no native support of Lets\u0026rsquo;s Encrypt The production deadline was getting close, but it was out of my religion to leave such an ugly solution. I was searching the Internet and bumped into Traefik. Integration was surprisingly fast and easy. I thought to myself: \u0026ldquo;That is how modern DevOps should look like\u0026rdquo;. It is been 3 years since I deal with Traefik, and today I wanna tell about one of the most much wished-for feature.\nStep by step setup üêü üêí ü¶ß ‚§¥Ô∏è üë∑üèº error-midl Traefik v2.x comes to us with middlewares and ErrorPage one of them.\nLett\u0026rsquo;s assume:\ntraefik dasboard domain: traefik-board.com your app: my-app.com 0001. Add labels to traefik. Important here traefik.http.routers.traefik.middlewares=maintenance-page@docker. If you try to look for https://service-does-not-exists.com you will get friendly error page.\ntraefik: deploy: labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.services.traefik.loadbalancer.server.port=8080\u0026#34; - \u0026#34;traefik.http.routers.traefik.rule=Host(`${traefik-board.com}`)\u0026#34; - \u0026#34;traefik.http.routers.traefik.entryPoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.traefik.tls=true\u0026#34; - \u0026#34;traefik.http.routers.traefik.middlewares=maintenance-page@docker\u0026#34; 0010. Create dedicated service for error page. Make sure service would be as highly available as Traefik! I recommend to set deploy mode: global and placement: constraints: - node.role == manager. If you don\u0026rsquo;t, hello boring black page again.\nmaintenance-page: deploy: labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.services.maintenance-page.loadbalancer.server.port=80\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.rule=HostRegexp(`{host:.+}`)\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.priority=1\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.middlewares=maintenance-page@docker\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.entrypoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.maintenance-page.tls=true\u0026#34; - \u0026#34;traefik.http.middlewares.maintenance-page.errors.status=400-599\u0026#34; - \u0026#34;traefik.http.middlewares.maintenance-page.errors.service=maintenance-page\u0026#34; - \u0026#34;traefik.http.middlewares.maintenance-page.errors.query=/\u0026#34; These lines are essential. You tell the server to catch all requests and redirect them to maintenance-page service. priority=1 means \u0026ldquo;all-catcher\u0026rdquo; is not alone in the system. There is someone higher who wants to handle requests firstly, for example, \u0026ldquo;all-catcher\u0026rdquo; for http-\u0026gt;https redirect.\ntraefik.http.routers.maintenance-page.priority=1 traefik.http.routers.maintenance-page.rule=HostRegexp(`{host:.+}`) 0011. Add priority=100 to app service.\napp: deploy: labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.frontend.rule=Host(`${my-app.com}`)\u0026#34; - \u0026#34;traefik.http.services.frontend.loadbalancer.server.port=80\u0026#34; - \u0026#34;traefik.http.routers.frontend.entrypoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.frontend.tls=true\u0026#34; - \u0026#34;traefik.http.routers.frontend.priority=100\u0026#34; Testing ü§πüèΩ‚Äç‚ôÄÔ∏è case 1: app service goes down or updating replicas: 0:1 -\u0026gt; see maitenance page case 2: user searched for https://service-does-not-exists.com -\u0026gt; see maitenance page Further reading üë©üèΩ‚Äçüíª hot thread about this topic on the Containous Community Forum nginx based error pages So, that‚Äôs all for now üòä. üë©‚ÄçüöÄ was happy to share my experience, and I\u0026rsquo;m sure this solution will save a bunch of time for someone. Happy hacking=) ü¶æ\n","permalink":"https://juliakostrikova.com/posts/cloud/traefik-error-page/","summary":"What you will learn today ü¶¶: what is Traefik and why I decisively recommend to use it with Docker Swarm how to show custom error page if Traefik returns 404 (service not found) How I met Traefik üòÅ Traefik is a top modern reverse proxy and load balancer. I started to use it from major version 1. At that moment, I already had Docker Swarm cluster, which I began to prepare for production use, and only one thing in the coherent picture has been left: \u0026ldquo;External Load Balancer\u0026rdquo;.","title":"Custom error page in Traefik v2.x. Finally"},{"content":"Task description on Codesignal\nI‚Äôve spend decent amount of time in order to figure out that I don‚Äôt need any string manipulation in this task üòÖ. My first try was adding missing zeros to the beginning of number slot, convert int to str and back, then the resulting string split by 4 but no luck ‚Ä¶ It turned out that is simple math üòÄ I wanna share my solution with detailed comments ‚¨áÔ∏è (Time complexity O(n) )\n\u0026lt;!-- Python code --\u0026gt; \u0026#39;\u0026#39;\u0026#39; Test cases a = [123, 4, 5] b = [100, 100, 100] a = [9876, 5432, 1999] b = [1, 8001] a = [1] b = [9999, 9999, 9999, 9999, 9999, 9999] \u0026#39;\u0026#39;\u0026#39; \u0026#39;\u0026#39;\u0026#39; helper \u0026#39;\u0026#39;\u0026#39; class ListNode: def __init__(self, x): self.value = x self.next = None \u0026#39;\u0026#39;\u0026#39; helper \u0026#39;\u0026#39;\u0026#39; def array_to_linked_list(arr): if not arr: return None head = ListNode(arr[0]) root = head for i in range(1, len(arr)): new = ListNode(arr[i]) head.next = new head = new return root \u0026#39;\u0026#39;\u0026#39; helper \u0026#39;\u0026#39;\u0026#39; def linked_to_array(l): res = [] while l is not None: res.append(l.value) l = l.next return res \u0026#39;\u0026#39;\u0026#39; helper \u0026#39;\u0026#39;\u0026#39; def reverse(l): prev = None head = l while head: next = head.next head.next = prev prev = head head = next return prev def addTwoHugeNumbers(a, b): \u0026#39;\u0026#39;\u0026#39; in order to proceed with addition from right to left we need to reverse linked list and when we finish reverse it back \u0026#39;\u0026#39;\u0026#39; a = reverse(a) b = reverse(b) \u0026#39;\u0026#39;\u0026#39; init resulting linked list and buffer \u0026#39;\u0026#39;\u0026#39; head = None root = None buffer = 0 while a or b: \u0026#39;\u0026#39;\u0026#39; set 0 if NodeList element is already None \u0026#39;\u0026#39;\u0026#39; a_value = 0 if not a else a.value b_value = 0 if not b else b.value sum_els = a_value + b_value if buffer != 0: sum_els = sum_els + buffer buffer = 0 \u0026#39;\u0026#39;\u0026#39; 10000 - condition to move to another number order buffer - always 1 because 9 is the highest value in decimal numeral system \u0026#39;\u0026#39;\u0026#39; if sum_els \u0026gt; 9999: sum_els = sum_els - 10000 buffer = 1 new = ListNode(sum_els) if head is None: head = new root = head else: head.next = new head = new a = None if not a else a.next b = None if not b else b.next \u0026#39;\u0026#39;\u0026#39; if we have not empty buffer at the end of addition then add one more NodeList to the beginning \u0026#39;\u0026#39;\u0026#39; if buffer != 0: new = ListNode(buffer) head.next = new return reverse(root) \u0026#39;\u0026#39;\u0026#39; we are going to do column addition like we do at school but with only difference: addition not one by one but four by four for example: 123, 4, 5 100,100,100 \u0026#39;\u0026#39;\u0026#39; a = array_to_linked_list(a) b = array_to_linked_list(b) result = addTwoHugeNumbers(a, b) print(linked_to_array(result)) ","permalink":"https://juliakostrikova.com/posts/algorithms/addtwohugenumbers/","summary":"Task description on Codesignal\nI‚Äôve spend decent amount of time in order to figure out that I don‚Äôt need any string manipulation in this task üòÖ. My first try was adding missing zeros to the beginning of number slot, convert int to str and back, then the resulting string split by 4 but no luck ‚Ä¶ It turned out that is simple math üòÄ I wanna share my solution with detailed comments ‚¨áÔ∏è (Time complexity O(n) )","title":"Solution for CodeSignal: addTwoHugeNumbers (Linked List part üëØ)"},{"content":"1 Add tools specifically to php monitoring in order to answer the questions ‚¨áÔ∏è what kind of php scripts were called most of the time? what kind of php scripts took the most CPU? We choose open-source tool Pinba.\nphp pinba extension sends data over UDP in protobuf format. pinba server accumulates and processes data and send it to data storage in your choice. (In our case it is Clickhouse DB)\nIt is not a secret that the most popular script in php frameworks is index.php but this information is barely helpful. So we need some ‚ÄúCPU statistics per route‚Äù.\n‚ÄúCPU statistics per route‚Äù means that: we have to implement one more chart in Grafana that shows CPU consumption in the matter of requested route, for example, V2\\Community\\Model.php And only after that we can give this info to developers for investigation what kind of code took the most CPU in the route: V2\\Community\\Model.php When the developer got the list of routes it is time to look inside those routes and investigate what cause high CPU. On this stage Blackfire can help. Blackfire will show you CPU Wall Time. To keep things simple, Wall Time is usually split in two main parts: the CPU Time and the I/O Time.\nThe CPU time is the amount of time the CPU was used for processing instructions. The I/O time is the time the CPU waited for input/output (I/O) operations.\nI/O time could be divided into two parts: the network and the disk.\nNetwork activity includes calls to databases like:\nMySQL, PostgreSQL, or MongoDB; HTTP calls to web services and APIs; Calls to cache systems like Redis and Memcached; Communications with services like queues, email daemons, remote filesystems; etc. Disk activity occurs when a program reads files from the filesystem, including loading code or files.\nWe need to distinguish these types of times in order to make further decisions related optimization.\nA CPU-bound program‚Äôs speed depends mostly on the CPU. In other words, CPU utilization is high for long periods of time. The faster the CPU, the faster the code runs. On the contrary, an I/O bound program‚Äôs speed is determined by the time spent waiting for I/O. Faster disks or a faster network improve the overall performance of I/O bound code.\n2 After that, depending on findings ‚¨áÔ∏è IF the reason of high CPU is I/O bound related to DB (too much time waiting for I/O ) then we should consider migrate to RDS and set up storage General Purpose (SSD) or Provisioned IOPS (SSD).\nIF the reason of high CPU : CPU-bound (which is rare case for web applications) then we should consider to optimize algorithms in code.\nto be continued‚Ä¶ üëÄ\n","permalink":"https://juliakostrikova.com/posts/php/high-cpu/","summary":"1 Add tools specifically to php monitoring in order to answer the questions ‚¨áÔ∏è what kind of php scripts were called most of the time? what kind of php scripts took the most CPU? We choose open-source tool Pinba.\nphp pinba extension sends data over UDP in protobuf format. pinba server accumulates and processes data and send it to data storage in your choice. (In our case it is Clickhouse DB)","title":"How to analyse ‚Äúhigh CPU‚Äù in php on production. Part 1"},{"content":"Into üíÜ Pre-conditions: macOS Sierra Docker CE Version 17.06.2-ce-mac27 PHP 7.1.8 XDebug v2.5.0 PhpStorm 2017.2 If you want to set up remote debug from your container you have 2 options:\nxdebug.remote_host = %host_ip% %host_ip% ‚Äî The IP of the machine running your IDE. It‚Äôs assumed on the same host as Docker. xdebug.remote_connect_back = on checks $_SERVER[‚ÄòHTTP_X_FORWARDED_FOR‚Äô] and $_SERVER[‚ÄòREMOTE_ADDR‚Äô] variables to find out which IP address to use. You cannot use xdebug.remote_connect_back because it takes containers‚Äôs internal IP. So xdebug.remote_host is the only option we have. It‚Äôs pretty simple to get host IP on Linux OS just look at : ifconfig \u0026gt; docker0. Moreover you can create a fixed set of IPs for both host and containers using docker network. For instance:\ndocker network create -d bridge \\ ‚Äî subnet 192.168.0.0/24 \\ ‚Äî gateway 192.168.0.1 \\ dockernet Now each container can connect to the host under the fixed IP 192.168.0.1 But the docker0 interface is not available for Docker in macOS. This interface is actually within HyperKit. So unfortunately there is no a reliable way of looping back to host for macOS. The only solution is to create your local IP‚Äôs alias to something else : sudo ifconfig lo0 alias 10.254.254.254\nHowTo guide üë©‚Äçüíª sudo ifconfig lo0 alias 10.254.254.254 (optional) For permanent setting up save.plist file sudo curl -o /Library/LaunchDaemons/com.yuklia.docker_localhost_alias.plist build docker containers \u0026hellip; add server to IDE storm1 configure Debug‚Äôs port in my case it‚Äôs 9002. xDebug client will knock to this port. storm2 configure DBGp Proxy storm3 Bonus üç∞ If you need to get a shell of the VM that was created by HyperKit run:\nscreen ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/tty or screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty For getting all screens run: screen -ls For exiting screen run : screen -X -S %screen_name% kill Once you get the tty running you can navigate to /var/lib/docker: storm4 Here is my Dockerfile and docker-compose versions for php:7-fpm with XDebug So that‚Äôs all for now üòä. Hope it will save a bunch of time for someone! I will be glad to get comments and questions. Happy hacking=) ü¶æ\n","permalink":"https://juliakostrikova.com/posts/php/xdebug-macos/","summary":"Into üíÜ Pre-conditions: macOS Sierra Docker CE Version 17.06.2-ce-mac27 PHP 7.1.8 XDebug v2.5.0 PhpStorm 2017.2 If you want to set up remote debug from your container you have 2 options:\nxdebug.remote_host = %host_ip% %host_ip% ‚Äî The IP of the machine running your IDE. It‚Äôs assumed on the same host as Docker. xdebug.remote_connect_back = on checks $_SERVER[‚ÄòHTTP_X_FORWARDED_FOR‚Äô] and $_SERVER[‚ÄòREMOTE_ADDR‚Äô] variables to find out which IP address to use. You cannot use xdebug.","title":"XDebug for PHP docker container on macOS"},{"content":"","permalink":"https://juliakostrikova.com/posts/misc/auto-rollout-terraform/","summary":"","title":""}]